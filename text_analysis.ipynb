{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading 15 million reviews \n",
    "\n",
    "*what is the best way to read 15 million reviews?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(file_name, head=100):\n",
    "    # 15739967\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            \n",
    "            # only get review_text and review_stars\n",
    "            data.append([d['review_text'], d['n_votes'], d['rating']])\n",
    "\n",
    "\n",
    "            # break if reaches the headth line\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.DataFrame(format_data(os.path.join(\n",
    "#     DIR, \"goodreads_reviews_dedup.json.gz\"))\n",
    "# )\n",
    "\n",
    "reviews = pd.read_csv(\"data/liwc_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>rating</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>32.86</td>\n",
       "      <td>27.51</td>\n",
       "      <td>39.59</td>\n",
       "      <td>65.53</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.79</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>358</td>\n",
       "      <td>63.49</td>\n",
       "      <td>35.27</td>\n",
       "      <td>64.13</td>\n",
       "      <td>45.44</td>\n",
       "      <td>17.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.80</td>\n",
       "      <td>5.31</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I haven't read a fun mystery book in a while a...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>22.00</td>\n",
       "      <td>6.98</td>\n",
       "      <td>91.21</td>\n",
       "      <td>95.15</td>\n",
       "      <td>12.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.28</td>\n",
       "      <td>6.98</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>23.12</td>\n",
       "      <td>10.38</td>\n",
       "      <td>83.40</td>\n",
       "      <td>27.64</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.64</td>\n",
       "      <td>5.03</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fun book that gives you a sense of living in...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>92.60</td>\n",
       "      <td>32.92</td>\n",
       "      <td>56.26</td>\n",
       "      <td>99.00</td>\n",
       "      <td>15.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.59</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tessa Bailey is known for writing the dirtiest...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>39.70</td>\n",
       "      <td>58.73</td>\n",
       "      <td>4.94</td>\n",
       "      <td>75.21</td>\n",
       "      <td>12.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.04</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4.5 stars!! Sweet Filthy Boy is the first book...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>88.48</td>\n",
       "      <td>19.55</td>\n",
       "      <td>61.86</td>\n",
       "      <td>85.37</td>\n",
       "      <td>17.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.29</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>This is an unexpectedly funny book with lots o...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>41.04</td>\n",
       "      <td>69.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>99.00</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.29</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>I liked this installment better than the first...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>32.41</td>\n",
       "      <td>14.43</td>\n",
       "      <td>66.26</td>\n",
       "      <td>84.42</td>\n",
       "      <td>13.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This novella gets 5 stars for the absolutely s...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>70.33</td>\n",
       "      <td>13.82</td>\n",
       "      <td>29.04</td>\n",
       "      <td>20.23</td>\n",
       "      <td>25.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.32</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_text  n_votes  rating  \\\n",
       "0    Mind blowingly cool. Best science fiction I've...       16       5   \n",
       "1    This is a special book. It started slow for ab...       28       5   \n",
       "2    I haven't read a fun mystery book in a while a...        6       3   \n",
       "3    Fun, fast paced, and disturbing tale of murder...       22       4   \n",
       "4    A fun book that gives you a sense of living in...        8       4   \n",
       "..                                                 ...      ...     ...   \n",
       "995  Tessa Bailey is known for writing the dirtiest...        0       5   \n",
       "996  4.5 stars!! Sweet Filthy Boy is the first book...        0       4   \n",
       "997  This is an unexpectedly funny book with lots o...        1       4   \n",
       "998  I liked this installment better than the first...        1       4   \n",
       "999  This novella gets 5 stars for the absolutely s...        5       3   \n",
       "\n",
       "     Segment   WC  Analytic  Clout  Authentic   Tone    WPS  ...  nonflu  \\\n",
       "0          1  174     32.86  27.51      39.59  65.53  19.33  ...     0.0   \n",
       "1          1  358     63.49  35.27      64.13  45.44  17.90  ...     0.0   \n",
       "2          1   86     22.00   6.98      91.21  95.15  12.29  ...     0.0   \n",
       "3          1  179     23.12  10.38      83.40  27.64  19.89  ...     0.0   \n",
       "4          1   79     92.60  32.92      56.26  99.00  15.80  ...     0.0   \n",
       "..       ...  ...       ...    ...        ...    ...    ...  ...     ...   \n",
       "995        1  255     39.70  58.73       4.94  75.21  12.75  ...     0.0   \n",
       "996        1  273     88.48  19.55      61.86  85.37  17.06  ...     0.0   \n",
       "997        1  179     41.04  69.35      29.07  99.00  19.89  ...     0.0   \n",
       "998        1   93     32.41  14.43      66.26  84.42  13.29  ...     0.0   \n",
       "999        1  126     70.33  13.82      29.04  20.23  25.20  ...     0.0   \n",
       "\n",
       "     filler  AllPunc  Period  Comma  QMark  Exclam  Apostro  OtherP  Emoji  \n",
       "0       0.0    13.79    5.17   2.30   0.00    0.00     2.87    3.45      0  \n",
       "1       0.0    14.80    5.31   3.07   1.12    0.00     0.84    4.47      0  \n",
       "2       0.0    16.28    6.98   2.33   0.00    1.16     4.65    1.16      0  \n",
       "3       0.0    15.64    5.03   2.79   0.00    0.00     1.12    6.70      0  \n",
       "4       0.0     7.59    6.33   0.00   0.00    0.00     0.00    1.27      0  \n",
       "..      ...      ...     ...    ...    ...     ...      ...     ...    ...  \n",
       "995     0.0    18.04    5.88   5.49   1.18    0.78     3.14    1.57      0  \n",
       "996     0.0    14.29    5.13   4.03   0.00    2.20     1.47    1.47      0  \n",
       "997     0.0    12.29    3.91   3.35   0.00    1.12     2.79    1.12      0  \n",
       "998     0.0    17.20    4.30   1.08   0.00    6.45     4.30    1.08      0  \n",
       "999     0.0    10.32    3.97   3.17   0.00    0.00     0.79    2.38      0  \n",
       "\n",
       "[1000 rows x 122 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(os.path.join(DIR, \"sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAG0CAYAAADdM0axAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAn5UlEQVR4nO3df3DU9Z3H8dcmuyG/SBZMOBIC+WHYokNIUrF66E1S2jtahjtLS42AHazCcUe0Xh2tlICKBVPEeFjBjkNCMVcpxlRaqoiIVG8g3uFJ+U2JISjEwCQZs0GSBnaze38w2bIFFDab7G4+z8eMM9nv95vd9/c7Onn6/X531+L1er0CAAAwRFSoBwAAABhIxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKNZQDxDO2tvb5Xa7Qz0GAAC4ClarVcOGDfvy7QZglojldrvlcrlCPQYAAAgiLnsBAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwChh9Vb3mpoa1dbW+i1LT0/XqlWrJEnnz59XdXW16urq5HK5lJ+fr7lz58put/u2b2tr09q1a3Xo0CHFxsaqqKhIs2bNUnR09ADuCQAACFdhFT+SNHr0aC1ZssT3OCrqryenXnrpJe3Zs0cPPfSQ4uPjVVVVpYqKCv3sZz+TJHk8HpWXl8tut2vZsmVqb2/X6tWrFR0drVmzZg34vgAAgPATdpe9oqKiZLfbff8kJSVJkrq6urRjxw7NmTNH48ePV05OjhYsWKCjR4+qvr5ekrRv3z41NTXpgQceUFZWlgoLC1VSUqK33nqLT2oGAACSwvDMz+nTpzV//nzZbDY5HA7NmjVLKSkpamxsVE9Pj/Ly8nzbjho1SikpKaqvr5fD4VB9fb3GjBnjdxmsoKBAlZWVOnnypLKzsy/7mi6Xy++TnC0Wi+Li4nw/AwCAwSOs4mfs2LFasGCB0tPT1d7ertraWj322GOqqKiQ0+mU1WpVQkKC3+8kJyfL6XRKkpxOp1/49K7vXXclmzZt8rvXKDs7WytWrFBqampQ9gsAAISPsIqfwsJC38+ZmZm+GHr//fcVExPTb687ffp0TZs2zfe492xPa2srl8sAAIgQVqv1qk5chFX8/K2EhASlp6fr9OnTmjBhgtxutzo7O/3O/nR0dPjO9tjtdjU0NPg9R0dHh2/dldhsNtlstsuu83q9fdsJAAAQVsLuhueLdXd36/Tp07Lb7crJyVF0dLQOHDjgW9/c3Ky2tjY5HA5JksPh0IkTJ3zBI0n79+9XXFycMjIyBnx+AAAQfsLqzE91dbUmTpyolJQUtbe3q6amRlFRUbr99tsVHx+vyZMnq7q6WomJiYqPj9e6devkcDh88ZOfn6+MjAytXr1as2fPltPp1MaNGzVlypQrntkBAABmsXjD6LrOqlWrdOTIEX3++edKSkrSuHHjdNddd2nkyJGS/vohh7t27ZLb7b7shxy2traqsrJShw4d0pAhQ1RUVKTZs2cH9CGHra2tfu8CAwBcm1OPzA31CGEhbWVlqEcwgs1mu6p7fsIqfsIN8QMAfUP8XED8DIyrjZ+wvucHAAAg2IgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUayhHuBKfve732nDhg2aOnWq7rnnHknS+fPnVV1drbq6OrlcLuXn52vu3Lmy2+2+32tra9PatWt16NAhxcbGqqioSLNmzVJ0dHRodgQAAISVsDzz09DQoLfffluZmZl+y1966SV9+OGHeuihh7R06VK1t7eroqLCt97j8ai8vFxut1vLli1TaWmp3n33Xb3yyisDvQsAACBMhd2Zn+7ubj3//POaP3++XnvtNd/yrq4u7dixQw8++KDGjx8vSVqwYIF+/OMfq76+Xg6HQ/v27VNTU5OWLFkiu92urKwslZSU6OWXX9add94pq/Xyu+tyueRyuXyPLRaL4uLifD8DANAX/C0JL2EXP5WVlSosLNSECRP84qexsVE9PT3Ky8vzLRs1apRSUlJ88VNfX68xY8b4XQYrKChQZWWlTp48qezs7Mu+5qZNm1RbW+t7nJ2drRUrVig1NTX4OwgABmkO9QBhIi0tLdQj4CJhFT+7du3S8ePHVV5efsk6p9Mpq9WqhIQEv+XJyclyOp2+bS4On971veuuZPr06Zo2bZrvcW+ht7a2yu12B7AnAAD81alTp0I9ghGsVutVnbgIm/hpa2vT+vXrtXjxYsXExAzoa9tsNtlstsuu83q9AzoLAGDw4W9JeAmb+GlsbFRHR4ceffRR3zKPx6MjR45o69atKisrk9vtVmdnp9/Zn46ODt/ZHrvdroaGBr/n7ejo8K0DAAAIm/jJy8vTM88847fsl7/8pdLT03XHHXcoJSVF0dHROnDggG699VZJUnNzs9ra2uRwOCRJDodDr732mjo6OnyXu/bv36+4uDhlZGQM7A4BAICwFDbxExcXpzFjxvgtGzJkiIYOHepbPnnyZFVXVysxMVHx8fFat26dHA6HL37y8/OVkZGh1atXa/bs2XI6ndq4caOmTJlyxctaAADALGETP1djzpw5slgsqqiokNvt9n3IYa+oqCgtXLhQlZWVWrx4sYYMGaKioiKVlJSEcGoAABBOLF7uwrqi1tZWv8//AQBcm1OPzP3yjQyQtrIy1CMYwWazXdW7vcLyE54BAAD6C/EDAACMQvwAAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwCvEDAACMQvwAAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwCvEDAACMQvwAAACjWEM9AAAA+GKnHpkb6hHCQtrKyqA8D2d+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEaxhnqAi23btk3btm1Ta2urJCkjI0MzZsxQYWGhJOn8+fOqrq5WXV2dXC6X8vPzNXfuXNntdt9ztLW1ae3atTp06JBiY2NVVFSkWbNmKTo6OhS7BAAAwkxYxc/w4cM1a9YspaWlyev16r333tPTTz+tp59+WqNHj9ZLL72kPXv26KGHHlJ8fLyqqqpUUVGhn/3sZ5Ikj8ej8vJy2e12LVu2TO3t7Vq9erWio6M1a9asEO8dAAAIB2F12WvixIn66le/qrS0NKWnp2vmzJmKjY3VRx99pK6uLu3YsUNz5szR+PHjlZOTowULFujo0aOqr6+XJO3bt09NTU164IEHlJWVpcLCQpWUlOitt96S2+0O8d4BAIBwEFZnfi7m8Xj0/vvv69y5c3I4HGpsbFRPT4/y8vJ824waNUopKSmqr6+Xw+FQfX29xowZ43cZrKCgQJWVlTp58qSys7Mv+1oul0sul8v32GKxKC4uzvczAAB9wd+S4AjWcQy7+Dlx4oTKysrkcrkUGxurhx9+WBkZGfr4449ltVqVkJDgt31ycrKcTqckyel0+oVP7/redVeyadMm1dbW+h5nZ2drxYoVSk1NDco+AYCpmkM9QJhIS0vr0+9zHC/o63HsFXbxk56erpUrV6qrq0v/8z//ozVr1mjp0qX9+prTp0/XtGnTfI97y7K1tZXLZQCAPjt16lSoRxgUvuw4Wq3WqzpxEXbxY7VaNXLkSElSTk6Ojh07pi1btmjSpElyu93q7Oz0O/vT0dHhO9tjt9vV0NDg93wdHR2+dVdis9lks9kuu87r9fZhbwAA4G9JsATrOIbVDc+X4/F45HK5lJOTo+joaB04cMC3rrm5WW1tbXI4HJIkh8OhEydO+IJHkvbv36+4uDhlZGQM+OwAACD8hNWZnw0bNqigoEApKSnq7u7Wzp07dfjwYZWVlSk+Pl6TJ09WdXW1EhMTFR8fr3Xr1snhcPjiJz8/XxkZGVq9erVmz54tp9OpjRs3asqUKVc8swMAAMwSVvHT0dGhNWvWqL29XfHx8crMzFRZWZkmTJggSZozZ44sFosqKirkdrt9H3LYKyoqSgsXLlRlZaUWL16sIUOGqKioSCUlJaHaJQAAEGYsXi5EXlFra6vfW+ABANfm1CNzv3wjA6StrOzT73McL/iy42iz2a7qhuewv+cHAAAgmAKOn/fee08tLS1XXN/S0qL33nsv0KcHAADoFwHHzwsvvOD7WonLaWho0AsvvBDo0wMAAPSLfrvs1d3dzTepAwCAsHNN7/b65JNP9PHHH/seHzlyRD09PZds19nZqbfffjtoH0MNAAAQLNcUP7t37/b7Dqzt27dr+/btl902Pj5e999/f9+mAwAACLJrip9vfvObuummm+T1erVo0SLdeeedKiwsvGS72NhY/d3f/R2XvQAAQNi5pvgZNmyYhg0bJkl6/PHHNWrUKN+3pgMAAESCgD/h+cYbbwzmHAAAAAOiT19vsXfvXu3YsUMtLS3q7Oy85NtWLRaLnn/++T4NCAAAEEwBx8/mzZv18ssvy2636/rrr9eYMWOCORcAAEC/CDh+tmzZovHjx+unP/2prNaw+n5UAACAKwr4Qw47Ozt16623Ej4AACCiBBw/ubm5am5uDuYsAAAA/S7g+Lnvvvu0e/du7dy5M5jzAAAA9KuAr1mtWrVKPT09ev7557V27Vpdd911iorybymLxaKVK1f2eUgAAIBgCTh+EhMTNXToUL6/CwAARJSA4+eJJ54I4hgAAAADI+B7fgAAACJRwGd+Dh8+fFXb8TUYAAAgnAQcP0uXLr2q7V555ZVAXwIAACDoAo6fxx9//JJlHo9HLS0teuedd+TxeDR79uw+DQcAABBs/fKt7sXFxXr88cd16NAhjR8/PtCXAAAACLp+ueE5KipKkyZN0o4dO/rj6QEAAALWb+/2Onv2rDo7O/vr6QEAAAIS8GWvtra2yy7v7OzUkSNHtHnzZt1www0BDwYAANAfAo6f0tLSL1w/duxYzZs3L9CnBwAA6BcBx8+///u/X7LMYrEoISFBI0eOVEZGRp8GAwAA6A8Bx09xcXEQxwAAABgYAcfPxZqamtTa2ipJSk1N5awPAAAIW32Knw8++EDV1dVqaWnxWz5ixAjNmTNHEydO7NNwAAAAwRZw/OzZs0cVFRVKTU3VzJkzfWd7mpqa9M477+iZZ57RwoULVVBQEKxZAQAA+izg+Pntb3+rzMxMLV26VLGxsb7lEydO1Le+9S099thjevXVV4kfAAAQVgL+kMMTJ06oqKjIL3x6xcbGqri4WCdOnOjTcAAAAMEWcPzYbDadPXv2iuvPnj0rm80W6NMDAAD0i4DjZ/z48dqyZYvq6+svWffRRx/pzTffVF5eXp+GAwAACLaA7/m5++67VVZWpiVLlig3N1fp6emSpObmZjU0NCg5OVmzZ88O2qAAAADBEHD8jBgxQs8884w2bdqkvXv3qq6uTtKFz/mZOnWqvvOd7yg5OTlogwIAAARDwPHT09Mjm82me+6557Lru7q61NPTo+jo6EBfAgAAIOgCvufnV7/6lZYsWXLF9UuWLFF1dXWgTw8AANAvAo6fvXv36pZbbrni+ltvvVV/+tOfAn16AACAfhFw/LS3t2v48OFXXD9s2DB99tlngT49AABAvwg4fhITE9Xc3HzF9Z9++qni4uICfXoAAIB+EXD8FBQUaPv27Tp+/Pgl6xobG7V9+3YVFhb2aTgAAIBgC/jdXiUlJdq7d68WLVqkm266SaNHj5YknTx5Uh9++KGSkpJUUlIStEEBAACCIeD4GT58uH7+85/r5Zdf1v/93//pgw8+kCTFxcXp9ttv18yZM7/wniAAAIBQCDh+pAs3Nd9///3yer06c+aMJCkpKUkWiyUowwEAAARbn+Knl8Vi4dOcAQBARAj4hmcAAIBIRPwAAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwSlC+2DRYNm3apN27d+vTTz9VTEyMHA6H7r77bqWnp/u2OX/+vKqrq1VXVyeXy6X8/HzNnTtXdrvdt01bW5vWrl2rQ4cOKTY2VkVFRZo1a5aio6NDsFcAACCchNWZn8OHD2vKlClavny5Fi9erJ6eHi1btkzd3d2+bV566SV9+OGHeuihh7R06VK1t7eroqLCt97j8ai8vFxut1vLli1TaWmp3n33Xb3yyiuh2CUAABBmwip+ysrKVFxcrNGjRysrK0ulpaVqa2tTY2OjJKmrq0s7duzQnDlzNH78eOXk5GjBggU6evSo6uvrJUn79u1TU1OTHnjgAWVlZamwsFAlJSV666235Ha7Q7l7AAAgDITVZa+/1dXVJUlKTEyUJDU2Nqqnp0d5eXm+bUaNGqWUlBTV19fL4XCovr5eY8aM8bsMVlBQoMrKSp08eVLZ2dmXvI7L5ZLL5fI9tlgsiouL8/0MAEBf8LckOIJ1HMM2fjwej9avX6+vfOUrGjNmjCTJ6XTKarUqISHBb9vk5GQ5nU7fNheHT+/63nWXs2nTJtXW1voeZ2dna8WKFUpNTQ3OzgCAoZpDPUCYSEtL69Pvcxwv6Otx7BW28VNVVaWTJ0/qySef7PfXmj59uqZNm+Z73FuWra2tXCoDAPTZqVOnQj3CoPBlx9FqtV7ViYuwjJ+qqirt2bNHS5cu1XXXXedbbrfb5Xa71dnZ6Xf2p6Ojw3e2x263q6Ghwe/5Ojo6fOsux2azyWazXXad1+vtw54AAMDfkmAJ1nEMqxuevV6vqqqqtHv3bj322GMaMWKE3/qcnBxFR0frwIEDvmXNzc1qa2uTw+GQJDkcDp04ccIXPJK0f/9+xcXFKSMjY2B2BAAAhK2wOvNTVVWlnTt36ic/+Yni4uJ89+jEx8crJiZG8fHxmjx5sqqrq5WYmKj4+HitW7dODofDFz/5+fnKyMjQ6tWrNXv2bDmdTm3cuFFTpky54tkdAABgjrCKn23btkmSnnjiCb/lCxYsUHFxsSRpzpw5slgsqqiokNvt9n3IYa+oqCgtXLhQlZWVWrx4sYYMGaKioiKVlJQM1G4AAIAwZvFyIfKKWltb/d4CDwC4NqcemfvlGxkgbWVln36f43jBlx1Hm812VTc8h9U9PwAAAP2N+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABjFGuoBLnb48GFt3rxZx48fV3t7ux5++GF97Wtf8633er2qqanRO++8o87OTo0bN05z585VWlqab5uzZ89q3bp1+vDDD2WxWHTLLbfohz/8oWJjY0OxSwAAIMyE1Zmfc+fOKSsrS/fdd99l1//+97/Xm2++qXnz5umpp57SkCFDtHz5cp0/f963zS9+8QudPHlSixcv1sKFC3XkyBG9+OKLA7ULAAAgzIXVmZ/CwkIVFhZedp3X69WWLVv03e9+VzfffLMk6f7779e8efP0wQcf6LbbblNTU5P27t2r8vJyXX/99ZKke++9V+Xl5frBD36g4cOHX/a5XS6XXC6X77HFYlFcXJzvZwAA+oK/JcERrOMYVvHzRVpaWuR0OjVhwgTfsvj4eOXm5qq+vl633Xab6uvrlZCQ4AsfScrLy5PFYlFDQ4PfJbSLbdq0SbW1tb7H2dnZWrFihVJTU/tvhwDAAM2hHiBMXHx7RiA4jhf09Tj2ipj4cTqdkqTk5GS/5cnJyb51TqdTSUlJfuujo6OVmJjo2+Zypk+frmnTpvke95Zla2ur3G5334cHABjt1KlToR5hUPiy42i1Wq/qxEXExE9/stlsstlsl13n9XoHeBoAwGDD35LgCNZxDKsbnr+I3W6XJHV0dPgt7+jo8K2z2+06c+aM3/qenh6dPXvWtw0AADBbxMTPiBEjZLfbdeDAAd+yrq4uNTQ0yOFwSJIcDoc6OzvV2Njo2+bgwYPyer3Kzc0d8JkBAED4CavLXt3d3Tp9+rTvcUtLiz7++GMlJiYqJSVFU6dO1Wuvvaa0tDSNGDFCGzdu1LBhw3zv/srIyFBBQYFefPFFzZs3T263W+vWrdOkSZOu+E4vAABglrCKn2PHjmnp0qW+x9XV1ZKkoqIilZaW6o477tC5c+f04osvqqurS+PGjdOiRYsUExPj+50f/ehHqqqq0pNPPun7kMN77713wPcFAACEJ4uXu7CuqLW11e/zfwAA1+bUI3NDPUJYSFtZ2aff5zhe8GXH0WazXdW7vSLmnh8AAIBgIH4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUcLqu70AIBzwVQIX9PUrGYBwxZkfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARuG7vQLEd/9cwHf/AAAiDWd+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABjFGuoBAATHqUfmhnqEsJC2sjLUIwAIc5z5AQAARiF+AACAUbjshZDjcs0FXK4BgIHBmR8AAGAU4gcAABhl0F722rp1q/7whz/I6XQqMzNT9957r3Jzc0M9FgAACLFBeeanrq5O1dXVmjFjhlasWKHMzEwtX75cHR0doR4NAACE2KCMn9dff13f+MY39PWvf10ZGRmaN2+eYmJi9Mc//jHUowEAgBAbdJe93G63Ghsb9Z3vfMe3LCoqSnl5eaqvr7/s77hcLrlcLt9ji8WiuLg4Wa1XPjxxWdcHbeZIZrPZ+vwcHMsL+nosOY4X8O9k8HAsg4f/voPjy47jF/3dvpjF6/V6gzFQuPjss8/0b//2b1q2bJkcDodv+a9//WsdPnxYTz311CW/U1NTo9raWt/j2267TQ8++OCAzAsAAAbWoLzsda2mT5+u9evX+/6ZN2+e35mgcPSXv/xFjz76qP7yl7+EepSIx7EMDo5j8HAsg4djGRyD7TgOusteSUlJioqKktPp9FvudDplt9sv+zs2my0op3cHktfr1fHjxzXITtyFBMcyODiOwcOxDB6OZXAMtuM46M78WK1W5eTk6ODBg75lHo9HBw8e9LsMBgAAzDTozvxI0rRp07RmzRrl5OQoNzdXW7Zs0blz51RcXBzq0QAAQIgNyviZNGmSzpw5o5qaGjmdTmVlZWnRokVXvOwViWw2m2bMmBFxl+vCEccyODiOwcOxDB6OZXAMtuM46N7tBQAA8EUG3T0/AAAAX4T4AQAARiF+AACAUYgfAABglEH5bq/BbuvWrfrDH/4gp9OpzMxM3XvvvcrNzQ31WBHn8OHD2rx5s44fP6729nY9/PDD+trXvhbqsSLOpk2btHv3bn366aeKiYmRw+HQ3XffrfT09FCPFnG2bdumbdu2qbW1VZKUkZGhGTNmqLCwMMSTRbbf/e532rBhg6ZOnap77rkn1ONElL/9+idJSk9P16pVq0IzUJAQPxGmrq5O1dXVmjdvnsaOHas33nhDy5cv16pVq5ScnBzq8SLKuXPnlJWVpcmTJ+uZZ54J9TgR6/Dhw5oyZYquv/569fT06De/+Y2WLVumZ599VrGxsaEeL6IMHz5cs2bNUlpamrxer9577z09/fTTevrppzV69OhQjxeRGhoa9PbbbyszMzPUo0Ss0aNHa8mSJb7HUVGRf9Eo8vfAMK+//rq+8Y1v6Otf/7oyMjI0b948xcTE6I9//GOoR4s4hYWFuuuuuzjb00dlZWUqLi7W6NGjlZWVpdLSUrW1tamxsTHUo0WciRMn6qtf/arS0tKUnp6umTNnKjY2Vh999FGoR4tI3d3dev755zV//nwlJCSEepyIFRUVJbvd7vsnKSkp1CP1GfETQdxutxobG5WXl+dbFhUVpby8PNXX14dwMuCvurq6JEmJiYkhniSyeTwe7dq1S+fOneOreQJUWVmpwsJCTZgwIdSjRLTTp09r/vz5uv/++/WLX/xCbW1toR6pz7jsFUHOnDkjj8dzySdV2+12NTc3h2Yo4CIej0fr16/XV77yFY0ZMybU40SkEydOqKysTC6XS7GxsXr44YeVkZER6rEizq5du3T8+HGVl5eHepSINnbsWC1YsEDp6elqb29XbW2tHnvsMVVUVCguLi7U4wWMMz8AgqaqqkonT57Uf/zHf4R6lIiVnp6ulStX6qmnntI//dM/ac2aNWpqagr1WBGlra1N69ev149+9CPFxMSEepyIVlhYqL//+79XZmamCgoK9NOf/lSdnZ16//33Qz1an3DmJ4IkJSUpKipKTqfTb7nT6RxU31uGyFRVVaU9e/Zo6dKluu6660I9TsSyWq0aOXKkJCknJ0fHjh3Tli1b9K//+q8hnixyNDY2qqOjQ48++qhvmcfj0ZEjR7R161Zt2LBhUNy0GwoJCQlKT0/X6dOnQz1KnxA/EcRqtSonJ0cHDx703aTr8Xh08OBBfetb3wrxdDCV1+vVunXrtHv3bj3xxBMaMWJEqEcaVDwej1wuV6jHiCh5eXmXvIPzl7/8pdLT03XHHXcQPn3Q3d2t06dP6x/+4R9CPUqfED8RZtq0aVqzZo1ycnKUm5urLVu26Ny5cyouLg71aBGn9z/iXi0tLfr444+VmJiolJSUEE4WWaqqqrRz50795Cc/UVxcnO/MZHx8PJccrtGGDRtUUFCglJQUdXd3a+fOnTp8+LDKyspCPVpEiYuLu+SesyFDhmjo0KHci3aNqqurNXHiRKWkpKi9vV01NTWKiorS7bffHurR+oT4iTCTJk3SmTNnVFNTI6fTqaysLC1atIjLXgE4duyYli5d6ntcXV0tSSoqKlJpaWmoxoo427ZtkyQ98cQTfssXLFhAlF+jjo4OrVmzRu3t7YqPj1dmZqbKysp4txJC5rPPPtNzzz2nzz//XElJSRo3bpyWL18e8W93t3i9Xm+ohwAAABgoXPgEAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AWCc0tJSrVmzJtRjAAgR4gfAoHT06FHV1NSos7Mz1KMACDN8txeAQeno0aOqra1VcXGxEhIS/NatWrVKFoslRJMBCDXO/ACIGN3d3UF5HpvNJquV//cDTMUXmwIISzU1NaqtrdWzzz6r3/72t9q7d69SU1NVWlqq119/XUeOHPF9+3lhYaF+8IMfaOjQoX6/+7dWr16tESNGqLS0VDfeeKNKS0slSe+++65eeOEFPfnkk/rf//1f/fd//7fOnz+vCRMmaP78+X7fYO3xeFRbW6t33nlHnZ2dGjt2rO677z6Vl5f7PSeA8MX/+gAIa88++6xGjhypmTNnyuv1av/+/WppaVFxcbHsdruampq0fft2NTU1afny5bJYLLrlllt06tQp7dq1S3PmzPFF0cURczm/+tWvlJCQoO9///tqaWnRli1bVFVVpR//+Me+bTZs2KDNmzfrpptuUn5+vj755BMtX75c58+f79fjACB4iB8AYS0zM1MPPvig7/H58+f1z//8z37bjB07Vs8995z+/Oc/64YbblBmZqays7O1a9cu3XzzzRoxYsRVvVZiYqIWL17sux/I6/XqzTffVFdXl+Lj4+V0OvXGG2/o5ptv1iOPPOL7vVdffVWvvvpqEPYWwEDgnh8AYe0f//Ef/R7HxMT4fj5//rzOnDmjsWPHSpKOHz/ep9f65je/6Xcj9A033CCPx6PW1lZJ0sGDB9XT06MpU6b4/d63v/3tPr0ugIHFmR8AYe1vz9qcPXtWr776qurq6tTR0eG3rqurq0+vlZKS4ve4911ivW+X742gkSNH+m2XmJh4yTvKAIQv4gdAWLv4TI8k/ed//qeOHj2qf/mXf1FWVpZiY2Pl8Xj01FNPyePx9Om1oqIufzKc94UAgwvxAyBinD17VgcOHNCdd96pGTNm+JafOnXqkm3743N8UlNTJUmnT5/2OyP1+eef82GKQAThnh8AEaP3zMzfnol54403Ltl2yJAhkvp+Kexi48ePV3R0tLZt2+a3fOvWrUF7DQD9jzM/ACJGfHy8brjhBm3evFk9PT0aPny49u3bp5aWlku2zcnJkST95je/0W233abo6GjddNNNio2NDfj17Xa7vv3tb+v111/XihUrVFBQoE8++UR/+tOfNHToUD41GogQnPkBEFEefPBB5efn66233tKGDRsUHR2tRYsWXbJdbm6uSkpK9Mknn2jNmjV67rnndObMmT6//t13363vfe97OnbsmP7rv/5Lp0+f1uLFiyVd+ORoAOGPT3gGgD7q7OzUD3/4Q91111367ne/G+pxAHwJzvwAwDW43Cc5995zdOONNw70OAACwD0/AHAN6urq9O6776qwsFCxsbH685//rF27dik/P1/jxo0L9XgArgLxAwDXYMyYMYqOjtbmzZvV1dUlu92uqVOn6q677gr1aACuEvf8AAAAo3DPDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwCvEDAACMQvwAAACjED8AAMAo/w+SX6EK4Rkf8QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(data=reviews, x='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful, beautifully written, poignant, subtle, and brilliant novel. \n",
      " This is the story of an English butler of one the great houses, whose self value comes from his duty and his dignity, and who questions his values and his life in the end. It is ultimately a story of regret. It is a story that makes you want to live your life and dedicate yourself to things that you won't regret when you look back. One of the best ways of thinking about regret I've ever come across is from Jeff Bezos in his nerdily titled Regret Minimization Framework. \n",
      " But what I really loved about the novel is how slow and forcefully it builds. I don't think I've read a book that does that so well since Pride and Prejudice. In the beginning you think it's just a butler going on a motoring trip. And at the end, your heart is breaking. \n",
      " Dignity is a big theme in the novel. Stevens attempts to define dignity several times throughout the novel - his definition generally has to do with self-respect and keeping ones emotions in check. But you have to wonder - his is suppression of emotions what leads him to where he ends up? His pushing himself to \"banter\" is a an interesting indication of him trying to break that pattern. \n",
      " \"We may now understand better, too, why my father was so fond of the story of the butler who failed to panic on discovering a tiger under the dining table; it was because he knew instinctively that somewhere in this story lay the kernel of what true 'dignity' is.\" \n",
      " \"What do you think dignity's all about?' The directness of the inquiry did, I admit, take me rather by surprise. 'It's rather a hard thing to explain in a few words, sir,' I said. 'But I suspect it comes down to not removing one's clothing in public.\" \n",
      " In the end, Stevens breaks your heart in two ways: he missed out on the love of his life, AND he realizes that he dedicated his life to serving a man who in the end wasn't the great man he thought he was. Good reminder: prioritize love first. And work on things that matter. \n",
      " Another aspect I loved is just learning about the era and how things in England worked. Fascinating for instance that multi-day stays at large estates were the only way to get a bunch of people together to communicate on an issue. Today, we communicate in very different ways. Seems appealing to try that way. \n",
      " I'll conclude this with another of my favorite quotes from the novel: \n",
      " You've got to enjoy yourself. The evening's the best part of the day. You've done your day's work. Now you can put your feet up and enjoy it. That's how I look at it. Ask anybody, they'll all tell you. The evening's the best part of the day.\n"
     ]
    }
   ],
   "source": [
    "example = reviews.iloc[55]['review_text']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A wonderful, beautifully written, poignant, subtle, and brilliant novel.',\n",
       " 'This is the story of an English butler of one the great houses, whose self value comes from his duty and his dignity, and who questions his values and his life in the end.',\n",
       " 'It is ultimately a story of regret.',\n",
       " \"It is a story that makes you want to live your life and dedicate yourself to things that you won't regret when you look back.\",\n",
       " \"One of the best ways of thinking about regret I've ever come across is from Jeff Bezos in his nerdily titled Regret Minimization Framework.\",\n",
       " 'But what I really loved about the novel is how slow and forcefully it builds.',\n",
       " \"I don't think I've read a book that does that so well since Pride and Prejudice.\",\n",
       " \"In the beginning you think it's just a butler going on a motoring trip.\",\n",
       " 'And at the end, your heart is breaking.',\n",
       " 'Dignity is a big theme in the novel.',\n",
       " 'Stevens attempts to define dignity several times throughout the novel - his definition generally has to do with self-respect and keeping ones emotions in check.',\n",
       " 'But you have to wonder - his is suppression of emotions what leads him to where he ends up?',\n",
       " 'His pushing himself to \"banter\" is a an interesting indication of him trying to break that pattern.',\n",
       " '\"We may now understand better, too, why my father was so fond of the story of the butler who failed to panic on discovering a tiger under the dining table; it was because he knew instinctively that somewhere in this story lay the kernel of what true \\'dignity\\' is.\"',\n",
       " '\"What do you think dignity\\'s all about?\\'',\n",
       " 'The directness of the inquiry did, I admit, take me rather by surprise.',\n",
       " \"'It's rather a hard thing to explain in a few words, sir,' I said.\",\n",
       " '\\'But I suspect it comes down to not removing one\\'s clothing in public.\"',\n",
       " \"In the end, Stevens breaks your heart in two ways: he missed out on the love of his life, AND he realizes that he dedicated his life to serving a man who in the end wasn't the great man he thought he was.\",\n",
       " 'Good reminder: prioritize love first.',\n",
       " 'And work on things that matter.',\n",
       " 'Another aspect I loved is just learning about the era and how things in England worked.',\n",
       " 'Fascinating for instance that multi-day stays at large estates were the only way to get a bunch of people together to communicate on an issue.',\n",
       " 'Today, we communicate in very different ways.',\n",
       " 'Seems appealing to try that way.',\n",
       " \"I'll conclude this with another of my favorite quotes from the novel: \\n You've got to enjoy yourself.\",\n",
       " \"The evening's the best part of the day.\",\n",
       " \"You've done your day's work.\",\n",
       " 'Now you can put your feet up and enjoy it.',\n",
       " \"That's how I look at it.\",\n",
       " \"Ask anybody, they'll all tell you.\",\n",
       " \"The evening's the best part of the day.\"]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(example)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_terms(sentence):\n",
    "    # Tokenize words and tag part of speech\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    sentiment_terms = set()\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        if word.lower() not in stop_words and word not in string.punctuation:\n",
    "            if tag.startswith('JJ') or tag.startswith('RB'):\n",
    "                if word.lower() in opinion_lexicon.positive() or word.lower() in opinion_lexicon.negative():\n",
    "                    sentiment_terms.add(word)\n",
    "\n",
    "    return sentiment_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_terms = set()\n",
    "for sentence in sentences:\n",
    "    sentiment_terms.update(extract_sentiment_terms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['well', 'brilliant', 'beautifully', 'self-respect', 'great']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_terms = list(sentiment_terms)\n",
    "sentiment_terms[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentiement terms from all reviews\n",
    "sentiment_terms = set()\n",
    "for review in reviews['review_text']:\n",
    "    sentences = sent_tokenize(review)\n",
    "    for sentence in sentences:\n",
    "        sentiment_terms.update(extract_sentiment_terms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_word_embeddings(processed_corpus):\n",
    "    # Train a Word2Vec model on the processed corpus\n",
    "    model = Word2Vec(sentences=processed_corpus,\n",
    "                     vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn_word_embeddings([word_tokenize(word) for word in sentiment_terms])\n",
    "\n",
    "token = 'amazing'\n",
    "\n",
    "if token in model.wv:\n",
    "    print(f\"{token}: {model.wv.most_similar(token)}\")\n",
    "else:\n",
    "    synonyms = get_synonyms(token)\n",
    "\n",
    "    for synonym in synonyms:\n",
    "        if synonym in model.wv:\n",
    "            print(f\"{synonym}: {model.wv.most_similar(synonym)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "\n",
    "parse, category_names = liwc.load_token_parser('data/LIWC2007_English100131.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 'incl', 'seven': 'number', 'years': 'relativ', 'ago': 'relativ', 'our': 'social', 'fathers': 'family', 'brought': 'relativ', 'on': 'relativ', 'this': 'ipron', 'a': 'article', 'new': 'relativ', 'nation': 'space', 'in': 'relativ', 'liberty': 'posemo', 'to': 'preps', 'the': 'article', 'that': 'ipron', 'all': 'certain', 'men': 'humans', 'are': 'present', 'created': 'achieve', 'equal': 'quant', 'we': 'incl', 'engaged': 'posemo', 'great': 'posemo', 'war': 'death', 'testing': 'work', 'whether': 'excl', 'or': 'excl', 'any': 'tentat', 'so': 'conj', 'can': 'present', 'long': 'relativ', 'met': 'social', 'battlefield': 'anger', 'of': 'preps', 'have': 'present', 'come': 'relativ', 'portion': 'quant', 'as': 'conj', 'final': 'time', 'resting': 'leisure', 'place': 'relativ', 'for': 'preps', 'those': 'ipron', 'who': 'social', 'here': 'adverb', 'gave': 'social', 'their': 'social', 'might': 'tentat', 'is': 'present', 'altogether': 'certain', 'should': 'discrep', 'do': 'present'}\n"
     ]
    }
   ],
   "source": [
    "# LIWC 2007\n",
    "from collections import Counter\n",
    "\n",
    "gettysburg = '''Four score and seven years ago our fathers brought forth on this continent a new nation, conceived in liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battlefield of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this.'''\n",
    "gettysburg_tokens = word_tokenize(gettysburg)\n",
    "# now flatmap over all the categories in all of the tokens using a generator:\n",
    "gettysburg_counts = {token: category for token in gettysburg_tokens for category in parse(token)}\n",
    "# and print the results:\n",
    "print(gettysburg_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = pd.read_csv('data/essays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/qth2myc91gj4tfy56qzfn3b40000gn/T/ipykernel_76067/1694335583.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  essays = essays.replace({'n': 0, 'y': 1})\n"
     ]
    }
   ],
   "source": [
    "essays = essays.rename(columns={\n",
    "    'TEXT': 'text',\n",
    "    'cEXT': 'extroversion',\n",
    "    'cNEU': 'neuroticism',\n",
    "    'cAGR': 'agreeableness',\n",
    "    'cCON': 'conscientiousness',\n",
    "    'cOPN': 'openness'\n",
    "})\n",
    "\n",
    "# subset the data text + big five\n",
    "\n",
    "essays = essays[['text', 'extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']]\n",
    "\n",
    "# convert n and y to 0 and 1\n",
    "essays = essays.replace({'n': 0, 'y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extroversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  extroversion  \\\n",
       "0     Well, right now I just woke up from a mid-day ...             0   \n",
       "1     Well, here we go with the stream of consciousn...             0   \n",
       "2     An open keyboard and buttons to push. The thin...             0   \n",
       "3     I can't believe it!  It's really happening!  M...             1   \n",
       "4     Well, here I go with the good old stream of co...             1   \n",
       "...                                                 ...           ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...             0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...             1   \n",
       "2464  It is Wednesday, December 8th and a lot has be...             0   \n",
       "2465  Man this week has been hellish. Anyways, now i...             0   \n",
       "2466  I have just gotten off the phone with brady. I...             0   \n",
       "\n",
       "      neuroticism  agreeableness  conscientiousness  openness  \n",
       "0               1              1                  0         1  \n",
       "1               0              1                  0         0  \n",
       "2               1              0                  1         1  \n",
       "3               0              1                  1         0  \n",
       "4               0              1                  0         1  \n",
       "...           ...            ...                ...       ...  \n",
       "2462            1              0                  1         0  \n",
       "2463            1              0                  0         1  \n",
       "2464            0              1                  0         0  \n",
       "2465            1              0                  0         1  \n",
       "2466            1              1                  0         1  \n",
       "\n",
       "[2467 rows x 6 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays.to_pickle('data/essays.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from preprocessing import preprocess_text\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load Pickled data\n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    df['preprocessed_text'] = df['text'].progress_apply(lambda x: ' '.join(map(str, preprocess_text(x)))) # type: ignore\n",
    "\n",
    "    df.to_pickle('data/essays_preprocessed.pkl')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(test, label):\n",
    "    \n",
    "    # OneVsRestClassifier with Logistic Regression\n",
    "\n",
    "    vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
    "    vectorizer.fit(test)\n",
    "\n",
    "    joblib.dump(vectorizer, 'models/vectorizer.pkl')\n",
    "\n",
    "    # transform the data\n",
    "    X = vectorizer.transform(test)\n",
    "\n",
    "    y = label\n",
    "\n",
    "    return X, y, vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_model_O(X_train, y_train):\n",
    "    # Model training implementation\n",
    "\n",
    "    model = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_model(X_train, y_train):\n",
    "    # Model training implementation\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_model(X_train, y_train):\n",
    "    # Model training implementation\n",
    "\n",
    "    model = SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_model_O(X_train, y_train):\n",
    "    # Model training implementation\n",
    "\n",
    "    model = OneVsRestClassifier(SVC())\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(model, X_test):\n",
    "    # Prediction implementation\n",
    "    return model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred):\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2467 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 1449/2467 [07:08<05:00,  3.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/essays.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 7\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load Pickled data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/essays_preprocessed.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[144], line 7\u001b[0m, in \u001b[0;36mload_data.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load Pickled data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/essays_preprocessed.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/dev/LCT/preprocessing.py:78\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m~/dev/LCT/preprocessing.py:35\u001b[0m, in \u001b[0;36mextract_sentiment_terms\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     32\u001b[0m sentiment_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m tagged_words:\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRB\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     37\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m opinion_lexicon\u001b[38;5;241m.\u001b[39mnegative() \u001b[38;5;129;01mor\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m opinion_lexicon\u001b[38;5;241m.\u001b[39mpositive():\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:199\u001b[0m, in \u001b[0;36mAbstractLazySequence.__contains__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:184\u001b[0m, in \u001b[0;36mAbstractLazySequence.count\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m elt \u001b[38;5;241m==\u001b[39m value)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:184\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/corpus/reader/util.py:302\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m, AbstractLazySequence)), (\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should return list or tuple.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    300\u001b[0m )\n\u001b[1;32m    301\u001b[0m num_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n\u001b[0;32m--> 302\u001b[0m new_filepos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtell\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m new_filepos \u001b[38;5;241m>\u001b[39m filepos, (\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock reader \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() should consume at least 1 byte (filepos=\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_block\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, filepos)\n\u001b[1;32m    306\u001b[0m )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Update our cache.\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/data.py:1291\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.tell\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# Calculate an estimate of where we think the newline is.\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m bytes_read \u001b[38;5;241m=\u001b[39m (orig_filepos \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbytebuffer)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewind_checkpoint\n\u001b[0;32m-> 1291\u001b[0m buf_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinebuffer)\n\u001b[1;32m   1292\u001b[0m est_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\n\u001b[1;32m   1293\u001b[0m     (bytes_read \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewind_numchars \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewind_numchars \u001b[38;5;241m+\u001b[39m buf_size))\n\u001b[1;32m   1294\u001b[0m )\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewind_checkpoint)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = load_data('data/essays.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/essays_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extroversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', ',', 'right', 'woke', 'mid-day', 'nap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['well', ',', 'go', 'stream', 'consciousness',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['open', 'keyboard', 'buttons', 'push', '.', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['ca', \"n't\", 'believe', '!', \"'s\", 'really', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', ',', 'go', 'good', 'old', 'stream', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"'m\", 'home', '.', 'wanted', 'go', 'bed', 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['stream', 'consiousnesssskdj', '.', 'spell', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['wednesday', ',', 'december', '8th', 'lot', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['man', 'week', 'hellish', '.', 'anyways', ','...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['gotten', 'phone', 'brady', '.', \"'m\", 'tryin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  extroversion  \\\n",
       "0     Well, right now I just woke up from a mid-day ...             0   \n",
       "1     Well, here we go with the stream of consciousn...             0   \n",
       "2     An open keyboard and buttons to push. The thin...             0   \n",
       "3     I can't believe it!  It's really happening!  M...             1   \n",
       "4     Well, here I go with the good old stream of co...             1   \n",
       "...                                                 ...           ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...             0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...             1   \n",
       "2464  It is Wednesday, December 8th and a lot has be...             0   \n",
       "2465  Man this week has been hellish. Anyways, now i...             0   \n",
       "2466  I have just gotten off the phone with brady. I...             0   \n",
       "\n",
       "      neuroticism  agreeableness  conscientiousness  openness  \\\n",
       "0               1              1                  0         1   \n",
       "1               0              1                  0         0   \n",
       "2               1              0                  1         1   \n",
       "3               0              1                  1         0   \n",
       "4               0              1                  0         1   \n",
       "...           ...            ...                ...       ...   \n",
       "2462            1              0                  1         0   \n",
       "2463            1              0                  0         1   \n",
       "2464            0              1                  0         0   \n",
       "2465            1              0                  0         1   \n",
       "2466            1              1                  0         1   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0     ['well', ',', 'right', 'woke', 'mid-day', 'nap...  \n",
       "1     ['well', ',', 'go', 'stream', 'consciousness',...  \n",
       "2     ['open', 'keyboard', 'buttons', 'push', '.', '...  \n",
       "3     ['ca', \"n't\", 'believe', '!', \"'s\", 'really', ...  \n",
       "4     ['well', ',', 'go', 'good', 'old', 'stream', '...  \n",
       "...                                                 ...  \n",
       "2462  [\"'m\", 'home', '.', 'wanted', 'go', 'bed', 're...  \n",
       "2463  ['stream', 'consiousnesssskdj', '.', 'spell', ...  \n",
       "2464  ['wednesday', ',', 'december', '8th', 'lot', '...  \n",
       "2465  ['man', 'week', 'hellish', '.', 'anyways', ','...  \n",
       "2466  ['gotten', 'phone', 'brady', '.', \"'m\", 'tryin...  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': {'accuracy': 0.05465587044534413,\n",
       "  'precision': 0.5918732510071519,\n",
       "  'recall': 0.6158583525789069,\n",
       "  'f1': 0.602117682675407}}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "X, y, vectorizer = extract_features(df['preprocessed_text'], df[['extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = train_svm_model_O(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "y_pred = predict_labels(model, X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred)\n",
    "\n",
    "results['Logistic Regression'] = {\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1': f1\n",
    "}\n",
    "\n",
    "joblib.dump(model, 'models/logistic_regression.pkl')\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.066802</td>\n",
       "      <td>0.588536</td>\n",
       "      <td>0.605851</td>\n",
       "      <td>0.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extroversion</th>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.551635</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.552023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neuroticism</th>\n",
       "      <td>0.574899</td>\n",
       "      <td>0.578739</td>\n",
       "      <td>0.574899</td>\n",
       "      <td>0.574690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agreeableness</th>\n",
       "      <td>0.528340</td>\n",
       "      <td>0.519925</td>\n",
       "      <td>0.528340</td>\n",
       "      <td>0.520523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conscientiousness</th>\n",
       "      <td>0.558704</td>\n",
       "      <td>0.561620</td>\n",
       "      <td>0.558704</td>\n",
       "      <td>0.559422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openness</th>\n",
       "      <td>0.623482</td>\n",
       "      <td>0.624988</td>\n",
       "      <td>0.623482</td>\n",
       "      <td>0.623581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accuracy  precision    recall        f1\n",
       "Logistic Regression  0.066802   0.588536  0.605851  0.595900\n",
       "extroversion         0.552632   0.551635  0.552632  0.552023\n",
       "neuroticism          0.574899   0.578739  0.574899  0.574690\n",
       "agreeableness        0.528340   0.519925  0.528340  0.520523\n",
       "conscientiousness    0.558704   0.561620  0.558704  0.559422\n",
       "openness             0.623482   0.624988  0.623482  0.623581"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trait = ['extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
    "\n",
    "for t in trait:\n",
    "    # Extract the features\n",
    "    X, y, vectorizer = extract_features(df['preprocessed_text'], df[t])\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model\n",
    "    model = train_logistic_model(X_train, y_train)\n",
    "\n",
    "    # Predict the labels\n",
    "    y_pred = predict_labels(model, X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred)\n",
    "    results[t] = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "    # Save the model\n",
    "    joblib.dump(model, f'models/{t}_model.pkl')\n",
    "\n",
    "# save the results\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df.to_csv('results/results.csv')\n",
    "\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'extroversion': 1,\n",
       " 'neuroticism': 1,\n",
       " 'agreeableness': 1,\n",
       " 'conscientiousness': 0,\n",
       " 'openness': 1}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"I am a very happy person.\"\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(f'models/logistic_regression.pkl')\n",
    "\n",
    "# Load the vectorizer\n",
    "vectorizer = joblib.load('models/vectorizer.pkl')\n",
    "\n",
    "# Preprocess the text\n",
    "test_sentence_processed = preprocess_text(test_sentence)\n",
    "if test_sentence_processed is not None:\n",
    "    test_sentence_processed = ' '.join(map(str, test_sentence_processed)) # type: ignore\n",
    "else:\n",
    "    print(\"Error: preprocess_text returned None\")\n",
    "\n",
    "# Vectorize the text\n",
    "test_sentence_vectorized = vectorizer.transform([test_sentence_processed])\n",
    "\n",
    "# Predict the label\n",
    "prediction = model.predict(test_sentence_vectorized)\n",
    "\n",
    "# format prediction to human readable OCEAN traits\n",
    "\n",
    "traits = ['extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
    "\n",
    "prediction = {trait: prediction[0][i] for i, trait in enumerate(traits)}\n",
    "\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
