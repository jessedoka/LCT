{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading 15 million reviews \n",
    "\n",
    "*what is the best way to read 15 million reviews?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(file_name, head=100):\n",
    "    # 15739967\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            \n",
    "            # only get review_text and review_stars\n",
    "            data.append([d['review_text'], d['n_votes'], d['rating']])\n",
    "\n",
    "\n",
    "            # break if reaches the headth line\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.DataFrame(format_data(os.path.join(\n",
    "#     DIR, \"goodreads_reviews_dedup.json.gz\"))\n",
    "# )\n",
    "\n",
    "reviews = pd.read_csv(\"data/liwc_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>rating</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>32.86</td>\n",
       "      <td>27.51</td>\n",
       "      <td>39.59</td>\n",
       "      <td>65.53</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.79</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>358</td>\n",
       "      <td>63.49</td>\n",
       "      <td>35.27</td>\n",
       "      <td>64.13</td>\n",
       "      <td>45.44</td>\n",
       "      <td>17.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.80</td>\n",
       "      <td>5.31</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I haven't read a fun mystery book in a while a...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>22.00</td>\n",
       "      <td>6.98</td>\n",
       "      <td>91.21</td>\n",
       "      <td>95.15</td>\n",
       "      <td>12.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.28</td>\n",
       "      <td>6.98</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>23.12</td>\n",
       "      <td>10.38</td>\n",
       "      <td>83.40</td>\n",
       "      <td>27.64</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.64</td>\n",
       "      <td>5.03</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fun book that gives you a sense of living in...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>92.60</td>\n",
       "      <td>32.92</td>\n",
       "      <td>56.26</td>\n",
       "      <td>99.00</td>\n",
       "      <td>15.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.59</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tessa Bailey is known for writing the dirtiest...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>39.70</td>\n",
       "      <td>58.73</td>\n",
       "      <td>4.94</td>\n",
       "      <td>75.21</td>\n",
       "      <td>12.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.04</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4.5 stars!! Sweet Filthy Boy is the first book...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>88.48</td>\n",
       "      <td>19.55</td>\n",
       "      <td>61.86</td>\n",
       "      <td>85.37</td>\n",
       "      <td>17.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.29</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>This is an unexpectedly funny book with lots o...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>41.04</td>\n",
       "      <td>69.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>99.00</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.29</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>I liked this installment better than the first...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>32.41</td>\n",
       "      <td>14.43</td>\n",
       "      <td>66.26</td>\n",
       "      <td>84.42</td>\n",
       "      <td>13.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This novella gets 5 stars for the absolutely s...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>70.33</td>\n",
       "      <td>13.82</td>\n",
       "      <td>29.04</td>\n",
       "      <td>20.23</td>\n",
       "      <td>25.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.32</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_text  n_votes  rating  \\\n",
       "0    Mind blowingly cool. Best science fiction I've...       16       5   \n",
       "1    This is a special book. It started slow for ab...       28       5   \n",
       "2    I haven't read a fun mystery book in a while a...        6       3   \n",
       "3    Fun, fast paced, and disturbing tale of murder...       22       4   \n",
       "4    A fun book that gives you a sense of living in...        8       4   \n",
       "..                                                 ...      ...     ...   \n",
       "995  Tessa Bailey is known for writing the dirtiest...        0       5   \n",
       "996  4.5 stars!! Sweet Filthy Boy is the first book...        0       4   \n",
       "997  This is an unexpectedly funny book with lots o...        1       4   \n",
       "998  I liked this installment better than the first...        1       4   \n",
       "999  This novella gets 5 stars for the absolutely s...        5       3   \n",
       "\n",
       "     Segment   WC  Analytic  Clout  Authentic   Tone    WPS  ...  nonflu  \\\n",
       "0          1  174     32.86  27.51      39.59  65.53  19.33  ...     0.0   \n",
       "1          1  358     63.49  35.27      64.13  45.44  17.90  ...     0.0   \n",
       "2          1   86     22.00   6.98      91.21  95.15  12.29  ...     0.0   \n",
       "3          1  179     23.12  10.38      83.40  27.64  19.89  ...     0.0   \n",
       "4          1   79     92.60  32.92      56.26  99.00  15.80  ...     0.0   \n",
       "..       ...  ...       ...    ...        ...    ...    ...  ...     ...   \n",
       "995        1  255     39.70  58.73       4.94  75.21  12.75  ...     0.0   \n",
       "996        1  273     88.48  19.55      61.86  85.37  17.06  ...     0.0   \n",
       "997        1  179     41.04  69.35      29.07  99.00  19.89  ...     0.0   \n",
       "998        1   93     32.41  14.43      66.26  84.42  13.29  ...     0.0   \n",
       "999        1  126     70.33  13.82      29.04  20.23  25.20  ...     0.0   \n",
       "\n",
       "     filler  AllPunc  Period  Comma  QMark  Exclam  Apostro  OtherP  Emoji  \n",
       "0       0.0    13.79    5.17   2.30   0.00    0.00     2.87    3.45      0  \n",
       "1       0.0    14.80    5.31   3.07   1.12    0.00     0.84    4.47      0  \n",
       "2       0.0    16.28    6.98   2.33   0.00    1.16     4.65    1.16      0  \n",
       "3       0.0    15.64    5.03   2.79   0.00    0.00     1.12    6.70      0  \n",
       "4       0.0     7.59    6.33   0.00   0.00    0.00     0.00    1.27      0  \n",
       "..      ...      ...     ...    ...    ...     ...      ...     ...    ...  \n",
       "995     0.0    18.04    5.88   5.49   1.18    0.78     3.14    1.57      0  \n",
       "996     0.0    14.29    5.13   4.03   0.00    2.20     1.47    1.47      0  \n",
       "997     0.0    12.29    3.91   3.35   0.00    1.12     2.79    1.12      0  \n",
       "998     0.0    17.20    4.30   1.08   0.00    6.45     4.30    1.08      0  \n",
       "999     0.0    10.32    3.97   3.17   0.00    0.00     0.79    2.38      0  \n",
       "\n",
       "[1000 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(os.path.join(DIR, \"sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj8AAAG0CAYAAADdM0axAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAn5UlEQVR4nO3df3DU9Z3H8dcmuyG/SBZMOBIC+WHYokNIUrF66E1S2jtahjtLS42AHazCcUe0Xh2tlICKBVPEeFjBjkNCMVcpxlRaqoiIVG8g3uFJ+U2JISjEwCQZs0GSBnaze38w2bIFFDab7G4+z8eMM9nv95vd9/c7Onn6/X531+L1er0CAAAwRFSoBwAAABhIxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKNZQDxDO2tvb5Xa7Qz0GAAC4ClarVcOGDfvy7QZglojldrvlcrlCPQYAAAgiLnsBAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwChh9Vb3mpoa1dbW+i1LT0/XqlWrJEnnz59XdXW16urq5HK5lJ+fr7lz58put/u2b2tr09q1a3Xo0CHFxsaqqKhIs2bNUnR09ADuCQAACFdhFT+SNHr0aC1ZssT3OCrqryenXnrpJe3Zs0cPPfSQ4uPjVVVVpYqKCv3sZz+TJHk8HpWXl8tut2vZsmVqb2/X6tWrFR0drVmzZg34vgAAgPATdpe9oqKiZLfbff8kJSVJkrq6urRjxw7NmTNH48ePV05OjhYsWKCjR4+qvr5ekrRv3z41NTXpgQceUFZWlgoLC1VSUqK33nqLT2oGAACSwvDMz+nTpzV//nzZbDY5HA7NmjVLKSkpamxsVE9Pj/Ly8nzbjho1SikpKaqvr5fD4VB9fb3GjBnjdxmsoKBAlZWVOnnypLKzsy/7mi6Xy++TnC0Wi+Li4nw/AwCAwSOs4mfs2LFasGCB0tPT1d7ertraWj322GOqqKiQ0+mU1WpVQkKC3+8kJyfL6XRKkpxOp1/49K7vXXclmzZt8rvXKDs7WytWrFBqampQ9gsAAISPsIqfwsJC38+ZmZm+GHr//fcVExPTb687ffp0TZs2zfe492xPa2srl8sAAIgQVqv1qk5chFX8/K2EhASlp6fr9OnTmjBhgtxutzo7O/3O/nR0dPjO9tjtdjU0NPg9R0dHh2/dldhsNtlstsuu83q9fdsJAAAQVsLuhueLdXd36/Tp07Lb7crJyVF0dLQOHDjgW9/c3Ky2tjY5HA5JksPh0IkTJ3zBI0n79+9XXFycMjIyBnx+AAAQfsLqzE91dbUmTpyolJQUtbe3q6amRlFRUbr99tsVHx+vyZMnq7q6WomJiYqPj9e6devkcDh88ZOfn6+MjAytXr1as2fPltPp1MaNGzVlypQrntkBAABmsXjD6LrOqlWrdOTIEX3++edKSkrSuHHjdNddd2nkyJGS/vohh7t27ZLb7b7shxy2traqsrJShw4d0pAhQ1RUVKTZs2cH9CGHra2tfu8CAwBcm1OPzA31CGEhbWVlqEcwgs1mu6p7fsIqfsIN8QMAfUP8XED8DIyrjZ+wvucHAAAg2IgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUayhHuBKfve732nDhg2aOnWq7rnnHknS+fPnVV1drbq6OrlcLuXn52vu3Lmy2+2+32tra9PatWt16NAhxcbGqqioSLNmzVJ0dHRodgQAAISVsDzz09DQoLfffluZmZl+y1966SV9+OGHeuihh7R06VK1t7eroqLCt97j8ai8vFxut1vLli1TaWmp3n33Xb3yyisDvQsAACBMhd2Zn+7ubj3//POaP3++XnvtNd/yrq4u7dixQw8++KDGjx8vSVqwYIF+/OMfq76+Xg6HQ/v27VNTU5OWLFkiu92urKwslZSU6OWXX9add94pq/Xyu+tyueRyuXyPLRaL4uLifD8DANAX/C0JL2EXP5WVlSosLNSECRP84qexsVE9PT3Ky8vzLRs1apRSUlJ88VNfX68xY8b4XQYrKChQZWWlTp48qezs7Mu+5qZNm1RbW+t7nJ2drRUrVig1NTX4OwgABmkO9QBhIi0tLdQj4CJhFT+7du3S8ePHVV5efsk6p9Mpq9WqhIQEv+XJyclyOp2+bS4On971veuuZPr06Zo2bZrvcW+ht7a2yu12B7AnAAD81alTp0I9ghGsVutVnbgIm/hpa2vT+vXrtXjxYsXExAzoa9tsNtlstsuu83q9AzoLAGDw4W9JeAmb+GlsbFRHR4ceffRR3zKPx6MjR45o69atKisrk9vtVmdnp9/Zn46ODt/ZHrvdroaGBr/n7ejo8K0DAAAIm/jJy8vTM88847fsl7/8pdLT03XHHXcoJSVF0dHROnDggG699VZJUnNzs9ra2uRwOCRJDodDr732mjo6OnyXu/bv36+4uDhlZGQM7A4BAICwFDbxExcXpzFjxvgtGzJkiIYOHepbPnnyZFVXVysxMVHx8fFat26dHA6HL37y8/OVkZGh1atXa/bs2XI6ndq4caOmTJlyxctaAADALGETP1djzpw5slgsqqiokNvt9n3IYa+oqCgtXLhQlZWVWrx4sYYMGaKioiKVlJSEcGoAABBOLF7uwrqi1tZWv8//AQBcm1OPzP3yjQyQtrIy1CMYwWazXdW7vcLyE54BAAD6C/EDAACMQvwAAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwCvEDAACMQvwAAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwCvEDAACMQvwAAACjWEM9AAAA+GKnHpkb6hHCQtrKyqA8D2d+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEaxhnqAi23btk3btm1Ta2urJCkjI0MzZsxQYWGhJOn8+fOqrq5WXV2dXC6X8vPzNXfuXNntdt9ztLW1ae3atTp06JBiY2NVVFSkWbNmKTo6OhS7BAAAwkxYxc/w4cM1a9YspaWlyev16r333tPTTz+tp59+WqNHj9ZLL72kPXv26KGHHlJ8fLyqqqpUUVGhn/3sZ5Ikj8ej8vJy2e12LVu2TO3t7Vq9erWio6M1a9asEO8dAAAIB2F12WvixIn66le/qrS0NKWnp2vmzJmKjY3VRx99pK6uLu3YsUNz5szR+PHjlZOTowULFujo0aOqr6+XJO3bt09NTU164IEHlJWVpcLCQpWUlOitt96S2+0O8d4BAIBwEFZnfi7m8Xj0/vvv69y5c3I4HGpsbFRPT4/y8vJ824waNUopKSmqr6+Xw+FQfX29xowZ43cZrKCgQJWVlTp58qSys7Mv+1oul0sul8v32GKxKC4uzvczAAB9wd+S4AjWcQy7+Dlx4oTKysrkcrkUGxurhx9+WBkZGfr4449ltVqVkJDgt31ycrKcTqckyel0+oVP7/redVeyadMm1dbW+h5nZ2drxYoVSk1NDco+AYCpmkM9QJhIS0vr0+9zHC/o63HsFXbxk56erpUrV6qrq0v/8z//ozVr1mjp0qX9+prTp0/XtGnTfI97y7K1tZXLZQCAPjt16lSoRxgUvuw4Wq3WqzpxEXbxY7VaNXLkSElSTk6Ojh07pi1btmjSpElyu93q7Oz0O/vT0dHhO9tjt9vV0NDg93wdHR2+dVdis9lks9kuu87r9fZhbwAA4G9JsATrOIbVDc+X4/F45HK5lJOTo+joaB04cMC3rrm5WW1tbXI4HJIkh8OhEydO+IJHkvbv36+4uDhlZGQM+OwAACD8hNWZnw0bNqigoEApKSnq7u7Wzp07dfjwYZWVlSk+Pl6TJ09WdXW1EhMTFR8fr3Xr1snhcPjiJz8/XxkZGVq9erVmz54tp9OpjRs3asqUKVc8swMAAMwSVvHT0dGhNWvWqL29XfHx8crMzFRZWZkmTJggSZozZ44sFosqKirkdrt9H3LYKyoqSgsXLlRlZaUWL16sIUOGqKioSCUlJaHaJQAAEGYsXi5EXlFra6vfW+ABANfm1CNzv3wjA6StrOzT73McL/iy42iz2a7qhuewv+cHAAAgmAKOn/fee08tLS1XXN/S0qL33nsv0KcHAADoFwHHzwsvvOD7WonLaWho0AsvvBDo0wMAAPSLfrvs1d3dzTepAwCAsHNN7/b65JNP9PHHH/seHzlyRD09PZds19nZqbfffjtoH0MNAAAQLNcUP7t37/b7Dqzt27dr+/btl902Pj5e999/f9+mAwAACLJrip9vfvObuummm+T1erVo0SLdeeedKiwsvGS72NhY/d3f/R2XvQAAQNi5pvgZNmyYhg0bJkl6/PHHNWrUKN+3pgMAAESCgD/h+cYbbwzmHAAAAAOiT19vsXfvXu3YsUMtLS3q7Oy85NtWLRaLnn/++T4NCAAAEEwBx8/mzZv18ssvy2636/rrr9eYMWOCORcAAEC/CDh+tmzZovHjx+unP/2prNaw+n5UAACAKwr4Qw47Ozt16623Ej4AACCiBBw/ubm5am5uDuYsAAAA/S7g+Lnvvvu0e/du7dy5M5jzAAAA9KuAr1mtWrVKPT09ev7557V27Vpdd911iorybymLxaKVK1f2eUgAAIBgCTh+EhMTNXToUL6/CwAARJSA4+eJJ54I4hgAAAADI+B7fgAAACJRwGd+Dh8+fFXb8TUYAAAgnAQcP0uXLr2q7V555ZVAXwIAACDoAo6fxx9//JJlHo9HLS0teuedd+TxeDR79uw+DQcAABBs/fKt7sXFxXr88cd16NAhjR8/PtCXAAAACLp+ueE5KipKkyZN0o4dO/rj6QEAAALWb+/2Onv2rDo7O/vr6QEAAAIS8GWvtra2yy7v7OzUkSNHtHnzZt1www0BDwYAANAfAo6f0tLSL1w/duxYzZs3L9CnBwAA6BcBx8+///u/X7LMYrEoISFBI0eOVEZGRp8GAwAA6A8Bx09xcXEQxwAAABgYAcfPxZqamtTa2ipJSk1N5awPAAAIW32Knw8++EDV1dVqaWnxWz5ixAjNmTNHEydO7NNwAAAAwRZw/OzZs0cVFRVKTU3VzJkzfWd7mpqa9M477+iZZ57RwoULVVBQEKxZAQAA+izg+Pntb3+rzMxMLV26VLGxsb7lEydO1Le+9S099thjevXVV4kfAAAQVgL+kMMTJ06oqKjIL3x6xcbGqri4WCdOnOjTcAAAAMEWcPzYbDadPXv2iuvPnj0rm80W6NMDAAD0i4DjZ/z48dqyZYvq6+svWffRRx/pzTffVF5eXp+GAwAACLaA7/m5++67VVZWpiVLlig3N1fp6emSpObmZjU0NCg5OVmzZ88O2qAAAADBEHD8jBgxQs8884w2bdqkvXv3qq6uTtKFz/mZOnWqvvOd7yg5OTlogwIAAARDwPHT09Mjm82me+6557Lru7q61NPTo+jo6EBfAgAAIOgCvufnV7/6lZYsWXLF9UuWLFF1dXWgTw8AANAvAo6fvXv36pZbbrni+ltvvVV/+tOfAn16AACAfhFw/LS3t2v48OFXXD9s2DB99tlngT49AABAvwg4fhITE9Xc3HzF9Z9++qni4uICfXoAAIB+EXD8FBQUaPv27Tp+/Pgl6xobG7V9+3YVFhb2aTgAAIBgC/jdXiUlJdq7d68WLVqkm266SaNHj5YknTx5Uh9++KGSkpJUUlIStEEBAACCIeD4GT58uH7+85/r5Zdf1v/93//pgw8+kCTFxcXp9ttv18yZM7/wniAAAIBQCDh+pAs3Nd9///3yer06c+aMJCkpKUkWiyUowwEAAARbn+Knl8Vi4dOcAQBARAj4hmcAAIBIRPwAAACjED8AAMAoxA8AADAK8QMAAIxC/AAAAKMQPwAAwCjEDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwSlC+2DRYNm3apN27d+vTTz9VTEyMHA6H7r77bqWnp/u2OX/+vKqrq1VXVyeXy6X8/HzNnTtXdrvdt01bW5vWrl2rQ4cOKTY2VkVFRZo1a5aio6NDsFcAACCchNWZn8OHD2vKlClavny5Fi9erJ6eHi1btkzd3d2+bV566SV9+OGHeuihh7R06VK1t7eroqLCt97j8ai8vFxut1vLli1TaWmp3n33Xb3yyiuh2CUAABBmwip+ysrKVFxcrNGjRysrK0ulpaVqa2tTY2OjJKmrq0s7duzQnDlzNH78eOXk5GjBggU6evSo6uvrJUn79u1TU1OTHnjgAWVlZamwsFAlJSV666235Ha7Q7l7AAAgDITVZa+/1dXVJUlKTEyUJDU2Nqqnp0d5eXm+bUaNGqWUlBTV19fL4XCovr5eY8aM8bsMVlBQoMrKSp08eVLZ2dmXvI7L5ZLL5fI9tlgsiouL8/0MAEBf8LckOIJ1HMM2fjwej9avX6+vfOUrGjNmjCTJ6XTKarUqISHBb9vk5GQ5nU7fNheHT+/63nWXs2nTJtXW1voeZ2dna8WKFUpNTQ3OzgCAoZpDPUCYSEtL69Pvcxwv6Otx7BW28VNVVaWTJ0/qySef7PfXmj59uqZNm+Z73FuWra2tXCoDAPTZqVOnQj3CoPBlx9FqtV7ViYuwjJ+qqirt2bNHS5cu1XXXXedbbrfb5Xa71dnZ6Xf2p6Ojw3e2x263q6Ghwe/5Ojo6fOsux2azyWazXXad1+vtw54AAMDfkmAJ1nEMqxuevV6vqqqqtHv3bj322GMaMWKE3/qcnBxFR0frwIEDvmXNzc1qa2uTw+GQJDkcDp04ccIXPJK0f/9+xcXFKSMjY2B2BAAAhK2wOvNTVVWlnTt36ic/+Yni4uJ89+jEx8crJiZG8fHxmjx5sqqrq5WYmKj4+HitW7dODofDFz/5+fnKyMjQ6tWrNXv2bDmdTm3cuFFTpky54tkdAABgjrCKn23btkmSnnjiCb/lCxYsUHFxsSRpzpw5slgsqqiokNvt9n3IYa+oqCgtXLhQlZWVWrx4sYYMGaKioiKVlJQM1G4AAIAwZvFyIfKKWltb/d4CDwC4NqcemfvlGxkgbWVln36f43jBlx1Hm812VTc8h9U9PwAAAP2N+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABjFGuoBLnb48GFt3rxZx48fV3t7ux5++GF97Wtf8633er2qqanRO++8o87OTo0bN05z585VWlqab5uzZ89q3bp1+vDDD2WxWHTLLbfohz/8oWJjY0OxSwAAIMyE1Zmfc+fOKSsrS/fdd99l1//+97/Xm2++qXnz5umpp57SkCFDtHz5cp0/f963zS9+8QudPHlSixcv1sKFC3XkyBG9+OKLA7ULAAAgzIXVmZ/CwkIVFhZedp3X69WWLVv03e9+VzfffLMk6f7779e8efP0wQcf6LbbblNTU5P27t2r8vJyXX/99ZKke++9V+Xl5frBD36g4cOHX/a5XS6XXC6X77HFYlFcXJzvZwAA+oK/JcERrOMYVvHzRVpaWuR0OjVhwgTfsvj4eOXm5qq+vl633Xab6uvrlZCQ4AsfScrLy5PFYlFDQ4PfJbSLbdq0SbW1tb7H2dnZWrFihVJTU/tvhwDAAM2hHiBMXHx7RiA4jhf09Tj2ipj4cTqdkqTk5GS/5cnJyb51TqdTSUlJfuujo6OVmJjo2+Zypk+frmnTpvke95Zla2ur3G5334cHABjt1KlToR5hUPiy42i1Wq/qxEXExE9/stlsstlsl13n9XoHeBoAwGDD35LgCNZxDKsbnr+I3W6XJHV0dPgt7+jo8K2z2+06c+aM3/qenh6dPXvWtw0AADBbxMTPiBEjZLfbdeDAAd+yrq4uNTQ0yOFwSJIcDoc6OzvV2Njo2+bgwYPyer3Kzc0d8JkBAED4CavLXt3d3Tp9+rTvcUtLiz7++GMlJiYqJSVFU6dO1Wuvvaa0tDSNGDFCGzdu1LBhw3zv/srIyFBBQYFefPFFzZs3T263W+vWrdOkSZOu+E4vAABglrCKn2PHjmnp0qW+x9XV1ZKkoqIilZaW6o477tC5c+f04osvqqurS+PGjdOiRYsUExPj+50f/ehHqqqq0pNPPun7kMN77713wPcFAACEJ4uXu7CuqLW11e/zfwAA1+bUI3NDPUJYSFtZ2aff5zhe8GXH0WazXdW7vSLmnh8AAIBgIH4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUcLqu70AIBzwVQIX9PUrGYBwxZkfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARuG7vQLEd/9cwHf/AAAiDWd+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AQAARiF+AACAUYgfAABgFOIHAAAYhfgBAABGIX4AAIBRiB8AAGAU4gcAABjFGuoBAATHqUfmhnqEsJC2sjLUIwAIc5z5AQAARiF+AACAUbjshZDjcs0FXK4BgIHBmR8AAGAU4gcAABhl0F722rp1q/7whz/I6XQqMzNT9957r3Jzc0M9FgAACLFBeeanrq5O1dXVmjFjhlasWKHMzEwtX75cHR0doR4NAACE2KCMn9dff13f+MY39PWvf10ZGRmaN2+eYmJi9Mc//jHUowEAgBAbdJe93G63Ghsb9Z3vfMe3LCoqSnl5eaqvr7/s77hcLrlcLt9ji8WiuLg4Wa1XPjxxWdcHbeZIZrPZ+vwcHMsL+nosOY4X8O9k8HAsg4f/voPjy47jF/3dvpjF6/V6gzFQuPjss8/0b//2b1q2bJkcDodv+a9//WsdPnxYTz311CW/U1NTo9raWt/j2267TQ8++OCAzAsAAAbWoLzsda2mT5+u9evX+/6ZN2+e35mgcPSXv/xFjz76qP7yl7+EepSIx7EMDo5j8HAsg4djGRyD7TgOusteSUlJioqKktPp9FvudDplt9sv+zs2my0op3cHktfr1fHjxzXITtyFBMcyODiOwcOxDB6OZXAMtuM46M78WK1W5eTk6ODBg75lHo9HBw8e9LsMBgAAzDTozvxI0rRp07RmzRrl5OQoNzdXW7Zs0blz51RcXBzq0QAAQIgNyviZNGmSzpw5o5qaGjmdTmVlZWnRokVXvOwViWw2m2bMmBFxl+vCEccyODiOwcOxDB6OZXAMtuM46N7tBQAA8EUG3T0/AAAAX4T4AQAARiF+AACAUYgfAABglEH5bq/BbuvWrfrDH/4gp9OpzMxM3XvvvcrNzQ31WBHn8OHD2rx5s44fP6729nY9/PDD+trXvhbqsSLOpk2btHv3bn366aeKiYmRw+HQ3XffrfT09FCPFnG2bdumbdu2qbW1VZKUkZGhGTNmqLCwMMSTRbbf/e532rBhg6ZOnap77rkn1ONElL/9+idJSk9P16pVq0IzUJAQPxGmrq5O1dXVmjdvnsaOHas33nhDy5cv16pVq5ScnBzq8SLKuXPnlJWVpcmTJ+uZZ54J9TgR6/Dhw5oyZYquv/569fT06De/+Y2WLVumZ599VrGxsaEeL6IMHz5cs2bNUlpamrxer9577z09/fTTevrppzV69OhQjxeRGhoa9PbbbyszMzPUo0Ss0aNHa8mSJb7HUVGRf9Eo8vfAMK+//rq+8Y1v6Otf/7oyMjI0b948xcTE6I9//GOoR4s4hYWFuuuuuzjb00dlZWUqLi7W6NGjlZWVpdLSUrW1tamxsTHUo0WciRMn6qtf/arS0tKUnp6umTNnKjY2Vh999FGoR4tI3d3dev755zV//nwlJCSEepyIFRUVJbvd7vsnKSkp1CP1GfETQdxutxobG5WXl+dbFhUVpby8PNXX14dwMuCvurq6JEmJiYkhniSyeTwe7dq1S+fOneOreQJUWVmpwsJCTZgwIdSjRLTTp09r/vz5uv/++/WLX/xCbW1toR6pz7jsFUHOnDkjj8dzySdV2+12NTc3h2Yo4CIej0fr16/XV77yFY0ZMybU40SkEydOqKysTC6XS7GxsXr44YeVkZER6rEizq5du3T8+HGVl5eHepSINnbsWC1YsEDp6elqb29XbW2tHnvsMVVUVCguLi7U4wWMMz8AgqaqqkonT57Uf/zHf4R6lIiVnp6ulStX6qmnntI//dM/ac2aNWpqagr1WBGlra1N69ev149+9CPFxMSEepyIVlhYqL//+79XZmamCgoK9NOf/lSdnZ16//33Qz1an3DmJ4IkJSUpKipKTqfTb7nT6RxU31uGyFRVVaU9e/Zo6dKluu6660I9TsSyWq0aOXKkJCknJ0fHjh3Tli1b9K//+q8hnixyNDY2qqOjQ48++qhvmcfj0ZEjR7R161Zt2LBhUNy0GwoJCQlKT0/X6dOnQz1KnxA/EcRqtSonJ0cHDx703aTr8Xh08OBBfetb3wrxdDCV1+vVunXrtHv3bj3xxBMaMWJEqEcaVDwej1wuV6jHiCh5eXmXvIPzl7/8pdLT03XHHXcQPn3Q3d2t06dP6x/+4R9CPUqfED8RZtq0aVqzZo1ycnKUm5urLVu26Ny5cyouLg71aBGn9z/iXi0tLfr444+VmJiolJSUEE4WWaqqqrRz50795Cc/UVxcnO/MZHx8PJccrtGGDRtUUFCglJQUdXd3a+fOnTp8+LDKyspCPVpEiYuLu+SesyFDhmjo0KHci3aNqqurNXHiRKWkpKi9vV01NTWKiorS7bffHurR+oT4iTCTJk3SmTNnVFNTI6fTqaysLC1atIjLXgE4duyYli5d6ntcXV0tSSoqKlJpaWmoxoo427ZtkyQ98cQTfssXLFhAlF+jjo4OrVmzRu3t7YqPj1dmZqbKysp4txJC5rPPPtNzzz2nzz//XElJSRo3bpyWL18e8W93t3i9Xm+ohwAAABgoXPgEAABGIX4AAIBRiB8AAGAU4gcAABiF+AEAAEYhfgAAgFGIHwAAYBTiBwAAGIX4AWCc0tJSrVmzJtRjAAgR4gfAoHT06FHV1NSos7Mz1KMACDN8txeAQeno0aOqra1VcXGxEhIS/NatWrVKFoslRJMBCDXO/ACIGN3d3UF5HpvNJquV//cDTMUXmwIISzU1NaqtrdWzzz6r3/72t9q7d69SU1NVWlqq119/XUeOHPF9+3lhYaF+8IMfaOjQoX6/+7dWr16tESNGqLS0VDfeeKNKS0slSe+++65eeOEFPfnkk/rf//1f/fd//7fOnz+vCRMmaP78+X7fYO3xeFRbW6t33nlHnZ2dGjt2rO677z6Vl5f7PSeA8MX/+gAIa88++6xGjhypmTNnyuv1av/+/WppaVFxcbHsdruampq0fft2NTU1afny5bJYLLrlllt06tQp7dq1S3PmzPFF0cURczm/+tWvlJCQoO9///tqaWnRli1bVFVVpR//+Me+bTZs2KDNmzfrpptuUn5+vj755BMtX75c58+f79fjACB4iB8AYS0zM1MPPvig7/H58+f1z//8z37bjB07Vs8995z+/Oc/64YbblBmZqays7O1a9cu3XzzzRoxYsRVvVZiYqIWL17sux/I6/XqzTffVFdXl+Lj4+V0OvXGG2/o5ptv1iOPPOL7vVdffVWvvvpqEPYWwEDgnh8AYe0f//Ef/R7HxMT4fj5//rzOnDmjsWPHSpKOHz/ep9f65je/6Xcj9A033CCPx6PW1lZJ0sGDB9XT06MpU6b4/d63v/3tPr0ugIHFmR8AYe1vz9qcPXtWr776qurq6tTR0eG3rqurq0+vlZKS4ve4911ivW+X742gkSNH+m2XmJh4yTvKAIQv4gdAWLv4TI8k/ed//qeOHj2qf/mXf1FWVpZiY2Pl8Xj01FNPyePx9Om1oqIufzKc94UAgwvxAyBinD17VgcOHNCdd96pGTNm+JafOnXqkm3743N8UlNTJUmnT5/2OyP1+eef82GKQAThnh8AEaP3zMzfnol54403Ltl2yJAhkvp+Kexi48ePV3R0tLZt2+a3fOvWrUF7DQD9jzM/ACJGfHy8brjhBm3evFk9PT0aPny49u3bp5aWlku2zcnJkST95je/0W233abo6GjddNNNio2NDfj17Xa7vv3tb+v111/XihUrVFBQoE8++UR/+tOfNHToUD41GogQnPkBEFEefPBB5efn66233tKGDRsUHR2tRYsWXbJdbm6uSkpK9Mknn2jNmjV67rnndObMmT6//t13363vfe97OnbsmP7rv/5Lp0+f1uLFiyVd+ORoAOGPT3gGgD7q7OzUD3/4Q91111367ne/G+pxAHwJzvwAwDW43Cc5995zdOONNw70OAACwD0/AHAN6urq9O6776qwsFCxsbH685//rF27dik/P1/jxo0L9XgArgLxAwDXYMyYMYqOjtbmzZvV1dUlu92uqVOn6q677gr1aACuEvf8AAAAo3DPDwAAMArxAwAAjEL8AAAAoxA/AADAKMQPAAAwCvEDAACMQvwAAACjED8AAMAo/w+SX6EK4Rkf8QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(data=reviews, x='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful, beautifully written, poignant, subtle, and brilliant novel. \n",
      " This is the story of an English butler of one the great houses, whose self value comes from his duty and his dignity, and who questions his values and his life in the end. It is ultimately a story of regret. It is a story that makes you want to live your life and dedicate yourself to things that you won't regret when you look back. One of the best ways of thinking about regret I've ever come across is from Jeff Bezos in his nerdily titled Regret Minimization Framework. \n",
      " But what I really loved about the novel is how slow and forcefully it builds. I don't think I've read a book that does that so well since Pride and Prejudice. In the beginning you think it's just a butler going on a motoring trip. And at the end, your heart is breaking. \n",
      " Dignity is a big theme in the novel. Stevens attempts to define dignity several times throughout the novel - his definition generally has to do with self-respect and keeping ones emotions in check. But you have to wonder - his is suppression of emotions what leads him to where he ends up? His pushing himself to \"banter\" is a an interesting indication of him trying to break that pattern. \n",
      " \"We may now understand better, too, why my father was so fond of the story of the butler who failed to panic on discovering a tiger under the dining table; it was because he knew instinctively that somewhere in this story lay the kernel of what true 'dignity' is.\" \n",
      " \"What do you think dignity's all about?' The directness of the inquiry did, I admit, take me rather by surprise. 'It's rather a hard thing to explain in a few words, sir,' I said. 'But I suspect it comes down to not removing one's clothing in public.\" \n",
      " In the end, Stevens breaks your heart in two ways: he missed out on the love of his life, AND he realizes that he dedicated his life to serving a man who in the end wasn't the great man he thought he was. Good reminder: prioritize love first. And work on things that matter. \n",
      " Another aspect I loved is just learning about the era and how things in England worked. Fascinating for instance that multi-day stays at large estates were the only way to get a bunch of people together to communicate on an issue. Today, we communicate in very different ways. Seems appealing to try that way. \n",
      " I'll conclude this with another of my favorite quotes from the novel: \n",
      " You've got to enjoy yourself. The evening's the best part of the day. You've done your day's work. Now you can put your feet up and enjoy it. That's how I look at it. Ask anybody, they'll all tell you. The evening's the best part of the day.\n"
     ]
    }
   ],
   "source": [
    "example = reviews.iloc[55]['review_text']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A wonderful, beautifully written, poignant, subtle, and brilliant novel.',\n",
       " 'This is the story of an English butler of one the great houses, whose self value comes from his duty and his dignity, and who questions his values and his life in the end.',\n",
       " 'It is ultimately a story of regret.',\n",
       " \"It is a story that makes you want to live your life and dedicate yourself to things that you won't regret when you look back.\",\n",
       " \"One of the best ways of thinking about regret I've ever come across is from Jeff Bezos in his nerdily titled Regret Minimization Framework.\",\n",
       " 'But what I really loved about the novel is how slow and forcefully it builds.',\n",
       " \"I don't think I've read a book that does that so well since Pride and Prejudice.\",\n",
       " \"In the beginning you think it's just a butler going on a motoring trip.\",\n",
       " 'And at the end, your heart is breaking.',\n",
       " 'Dignity is a big theme in the novel.',\n",
       " 'Stevens attempts to define dignity several times throughout the novel - his definition generally has to do with self-respect and keeping ones emotions in check.',\n",
       " 'But you have to wonder - his is suppression of emotions what leads him to where he ends up?',\n",
       " 'His pushing himself to \"banter\" is a an interesting indication of him trying to break that pattern.',\n",
       " '\"We may now understand better, too, why my father was so fond of the story of the butler who failed to panic on discovering a tiger under the dining table; it was because he knew instinctively that somewhere in this story lay the kernel of what true \\'dignity\\' is.\"',\n",
       " '\"What do you think dignity\\'s all about?\\'',\n",
       " 'The directness of the inquiry did, I admit, take me rather by surprise.',\n",
       " \"'It's rather a hard thing to explain in a few words, sir,' I said.\",\n",
       " '\\'But I suspect it comes down to not removing one\\'s clothing in public.\"',\n",
       " \"In the end, Stevens breaks your heart in two ways: he missed out on the love of his life, AND he realizes that he dedicated his life to serving a man who in the end wasn't the great man he thought he was.\",\n",
       " 'Good reminder: prioritize love first.',\n",
       " 'And work on things that matter.',\n",
       " 'Another aspect I loved is just learning about the era and how things in England worked.',\n",
       " 'Fascinating for instance that multi-day stays at large estates were the only way to get a bunch of people together to communicate on an issue.',\n",
       " 'Today, we communicate in very different ways.',\n",
       " 'Seems appealing to try that way.',\n",
       " \"I'll conclude this with another of my favorite quotes from the novel: \\n You've got to enjoy yourself.\",\n",
       " \"The evening's the best part of the day.\",\n",
       " \"You've done your day's work.\",\n",
       " 'Now you can put your feet up and enjoy it.',\n",
       " \"That's how I look at it.\",\n",
       " \"Ask anybody, they'll all tell you.\",\n",
       " \"The evening's the best part of the day.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(example)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_terms(sentence):\n",
    "    # Tokenize words and tag part of speech\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    sentiment_terms = set()\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        if word.lower() not in stop_words and word not in string.punctuation:\n",
    "            if tag.startswith('JJ') or tag.startswith('RB'):\n",
    "                if word.lower() in opinion_lexicon.positive() or word.lower() in opinion_lexicon.negative():\n",
    "                    sentiment_terms.add(word)\n",
    "\n",
    "    return sentiment_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_terms = set()\n",
    "for sentence in sentences:\n",
    "    sentiment_terms.update(extract_sentiment_terms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautifully', 'wonderful', 'poignant', 'self-respect', 'hard']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_terms = list(sentiment_terms)\n",
    "sentiment_terms[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_word_embeddings(processed_corpus):\n",
    "    # Train a Word2Vec model on the processed corpus\n",
    "    model = Word2Vec(sentences=processed_corpus,\n",
    "                     vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learn_word_embeddings([word_tokenize(word) for word in sentiment_terms])\n",
    "\n",
    "token = 'amazing'\n",
    "\n",
    "if token in model.wv:\n",
    "    print(f\"{token}: {model.wv.most_similar(token)}\")\n",
    "else:\n",
    "    synonyms = get_synonyms(token)\n",
    "\n",
    "    for synonym in synonyms:\n",
    "        if synonym in model.wv:\n",
    "            print(f\"{synonym}: {model.wv.most_similar(synonym)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = pd.read_csv('data/essays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/qth2myc91gj4tfy56qzfn3b40000gn/T/ipykernel_10593/1694335583.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  essays = essays.replace({'n': 0, 'y': 1})\n"
     ]
    }
   ],
   "source": [
    "essays = essays.rename(columns={\n",
    "    'TEXT': 'text',\n",
    "    'cEXT': 'extroversion',\n",
    "    'cNEU': 'neuroticism',\n",
    "    'cAGR': 'agreeableness',\n",
    "    'cCON': 'conscientiousness',\n",
    "    'cOPN': 'openness'\n",
    "})\n",
    "\n",
    "# subset the data text + big five\n",
    "\n",
    "essays = essays[['text', 'extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']]\n",
    "\n",
    "# convert n and y to 0 and 1\n",
    "essays = essays.replace({'n': 0, 'y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extroversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  extroversion  \\\n",
       "0     Well, right now I just woke up from a mid-day ...             0   \n",
       "1     Well, here we go with the stream of consciousn...             0   \n",
       "2     An open keyboard and buttons to push. The thin...             0   \n",
       "3     I can't believe it!  It's really happening!  M...             1   \n",
       "4     Well, here I go with the good old stream of co...             1   \n",
       "...                                                 ...           ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...             0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...             1   \n",
       "2464  It is Wednesday, December 8th and a lot has be...             0   \n",
       "2465  Man this week has been hellish. Anyways, now i...             0   \n",
       "2466  I have just gotten off the phone with brady. I...             0   \n",
       "\n",
       "      neuroticism  agreeableness  conscientiousness  openness  \n",
       "0               1              1                  0         1  \n",
       "1               0              1                  0         0  \n",
       "2               1              0                  1         1  \n",
       "3               0              1                  1         0  \n",
       "4               0              1                  0         1  \n",
       "...           ...            ...                ...       ...  \n",
       "2462            1              0                  1         0  \n",
       "2463            1              0                  0         1  \n",
       "2464            0              1                  0         0  \n",
       "2465            1              0                  0         1  \n",
       "2466            1              1                  0         1  \n",
       "\n",
       "[2467 rows x 6 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays.to_pickle('data/essays.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from preprocessing import preprocess_text\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load Pickled data\n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    df['preprocessed_text'] = df['text'].progress_apply(lambda x: ' '.join(map(str, preprocess_text(x)))) # type: ignore\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred):\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, target_columns, model, model_path):\n",
    "\n",
    "    X = df['preprocessed_text']\n",
    "    y = df[target_columns]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred)\n",
    "\n",
    "    if model_path is not None:\n",
    "        joblib.dump(model, model_path)\n",
    "\n",
    "    return [model, accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 39/2467 [00:11<12:19,  3.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/essays.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load Pickled data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36mload_data.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load Pickled data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/dev/LCT/preprocessing.py:88\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     85\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sentence)])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Extract sentiment terms\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m sentiment_terms \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sentiment_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Tokenize words\u001b[39;00m\n\u001b[1;32m     91\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n",
      "File \u001b[0;32m~/dev/LCT/preprocessing.py:37\u001b[0m, in \u001b[0;36mextract_sentiment_terms\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRB\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopinion_lexicon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnegative\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m opinion_lexicon\u001b[38;5;241m.\u001b[39mpositive():\n\u001b[1;32m     38\u001b[0m                 sentiment_terms\u001b[38;5;241m.\u001b[39madd(word)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_terms\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:199\u001b[0m, in \u001b[0;36mAbstractLazySequence.__contains__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:184\u001b[0m, in \u001b[0;36mAbstractLazySequence.count\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m elt \u001b[38;5;241m==\u001b[39m value)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:184\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = load_data('data/essays.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/essays_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extroversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', ',', 'right', 'woke', 'mid-day', 'nap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['well', ',', 'go', 'stream', 'consciousness',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['open', 'keyboard', 'buttons', 'push', '.', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['ca', \"n't\", 'believe', '!', \"'s\", 'really', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', ',', 'go', 'good', 'old', 'stream', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"'m\", 'home', '.', 'wanted', 'go', 'bed', 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['stream', 'consiousnesssskdj', '.', 'spell', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['wednesday', ',', 'december', '8th', 'lot', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['man', 'week', 'hellish', '.', 'anyways', ','...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['gotten', 'phone', 'brady', '.', \"'m\", 'tryin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  extroversion  \\\n",
       "0     Well, right now I just woke up from a mid-day ...             0   \n",
       "1     Well, here we go with the stream of consciousn...             0   \n",
       "2     An open keyboard and buttons to push. The thin...             0   \n",
       "3     I can't believe it!  It's really happening!  M...             1   \n",
       "4     Well, here I go with the good old stream of co...             1   \n",
       "...                                                 ...           ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...             0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...             1   \n",
       "2464  It is Wednesday, December 8th and a lot has be...             0   \n",
       "2465  Man this week has been hellish. Anyways, now i...             0   \n",
       "2466  I have just gotten off the phone with brady. I...             0   \n",
       "\n",
       "      neuroticism  agreeableness  conscientiousness  openness  \\\n",
       "0               1              1                  0         1   \n",
       "1               0              1                  0         0   \n",
       "2               1              0                  1         1   \n",
       "3               0              1                  1         0   \n",
       "4               0              1                  0         1   \n",
       "...           ...            ...                ...       ...   \n",
       "2462            1              0                  1         0   \n",
       "2463            1              0                  0         1   \n",
       "2464            0              1                  0         0   \n",
       "2465            1              0                  0         1   \n",
       "2466            1              1                  0         1   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0     ['well', ',', 'right', 'woke', 'mid-day', 'nap...  \n",
       "1     ['well', ',', 'go', 'stream', 'consciousness',...  \n",
       "2     ['open', 'keyboard', 'buttons', 'push', '.', '...  \n",
       "3     ['ca', \"n't\", 'believe', '!', \"'s\", 'really', ...  \n",
       "4     ['well', ',', 'go', 'good', 'old', 'stream', '...  \n",
       "...                                                 ...  \n",
       "2462  [\"'m\", 'home', '.', 'wanted', 'go', 'bed', 're...  \n",
       "2463  ['stream', 'consiousnesssskdj', '.', 'spell', ...  \n",
       "2464  ['wednesday', ',', 'december', '8th', 'lot', '...  \n",
       "2465  ['man', 'week', 'hellish', '.', 'anyways', ','...  \n",
       "2466  ['gotten', 'phone', 'brady', '.', \"'m\", 'tryin...  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = ['extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"logistic_regression\"] = train_model(df, traits, model, 'models/big_five_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "results[\"logistic_regression improved\"] = train_model(df, traits, grid_search, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Model Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Training model for neuroticism\n",
      "Training model for agreeableness\n",
      "Training model for conscientiousness\n",
      "Training model for openness\n"
     ]
    }
   ],
   "source": [
    "results[\"LR Multi-Model\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"LR Multi-Model\"][t] = train_model(df, t, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for neuroticism\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for agreeableness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for conscientiousness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for openness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "results[\"LR Multi-Model Classification Improved\"] = {}\n",
    "\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"LR Multi-Model Classification Improved\"][t] = train_model(df, t, grid_search, None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(dual=True)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"svm\"] = train_model(df, traits, model, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OneVsRestClassifier is a simple way to train a multi-label classification model.\n",
    "\n",
    "But its performance is not good enough. with accuracy of 0.07. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC(dual=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Training model for neuroticism\n",
      "Training model for agreeableness\n",
      "Training model for conscientiousness\n",
      "Training model for openness\n"
     ]
    }
   ],
   "source": [
    "results[\"SVM Multi Model\"] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(12273) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12274) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12275) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12276) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12278) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12279) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12280) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Python(12281) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm_model, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "svm_models = [(t, train_model(df, t, grid_search, None)[0]) for t in traits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [1]\n",
      "1: [1]\n",
      "2: [1]\n",
      "3: [1]\n",
      "4: [1]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"fndoengiqnonq\"\n",
    "models = [(i, train_model(df, t, svm_model, None)[0]) for i, t in enumerate(traits)]\n",
    "\n",
    "for i, model in svm_models:\n",
    "\n",
    "    test_sentence_processed = preprocess_text(test_sentence)\n",
    "    test_sentence_processed = ' '.join(map(str, test_sentence_processed)) # type: ignore\n",
    "    \n",
    "    prediction = model.predict([test_sentence])\n",
    "    print(f'{i}: {prediction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randmom Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for neuroticism\n",
      "Training model for agreeableness\n",
      "Training model for conscientiousness\n",
      "Training model for openness\n"
     ]
    }
   ],
   "source": [
    "results[\"Random Forest\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"Random Forest\"][t] = train_model(df, t, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m traits:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom Forest Improved\u001b[39m\u001b[38;5;124m\"\u001b[39m][t] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_search\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(df, target_columns, model, model_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target_columns]\n\u001b[1;32m      6\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     12\u001b[0m accuracy, precision, recall, f1 \u001b[38;5;241m=\u001b[39m evaluate_model(y_test, y_pred)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__n_estimators': [50, 100, 200],\n",
    "    'clf__max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "results[\"Random Forest Improved\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"Random Forest Improved\"][t] = train_model(df, t, grid_search, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                  ('clf',\n",
       "                   OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000)))]),\n",
       "  0.0728744939271255,\n",
       "  0.5926256141521749,\n",
       "  0.625866050808314,\n",
       "  0.6079697557341297],\n",
       " 'logistic_regression improved': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                         ('clf',\n",
       "                                          OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000)))]),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'clf__estimator__C': [0.1, 1, 10],\n",
       "                           'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                           'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "               verbose=1),\n",
       "  0.0728744939271255,\n",
       "  0.5953794656683132,\n",
       "  0.6289453425712086,\n",
       "  0.6106791071582232],\n",
       " 'LR Multi-Model': {'extroversion': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5566801619433198,\n",
       "   0.5554302204838598,\n",
       "   0.5566801619433198,\n",
       "   0.5558710736548559],\n",
       "  'neuroticism': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5910931174089069,\n",
       "   0.5958157512391467,\n",
       "   0.5910931174089069,\n",
       "   0.5906372344712552],\n",
       "  'agreeableness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5647773279352226,\n",
       "   0.557906411630976,\n",
       "   0.5647773279352226,\n",
       "   0.5570403080475619],\n",
       "  'conscientiousness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5587044534412956,\n",
       "   0.5599262454805568,\n",
       "   0.5587044534412956,\n",
       "   0.5591693344344936],\n",
       "  'openness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.6093117408906883,\n",
       "   0.6095201472791609,\n",
       "   0.6093117408906883,\n",
       "   0.6093934859017636]},\n",
       " 'LR Multi-Model Classification Improved': {'extroversion': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5607287449392713,\n",
       "   0.5597579044083079,\n",
       "   0.5607287449392713,\n",
       "   0.5601311266298393],\n",
       "  'neuroticism': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6093117408906883,\n",
       "   0.6221957755487011,\n",
       "   0.6093117408906883,\n",
       "   0.6055588847875354],\n",
       "  'agreeableness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5850202429149798,\n",
       "   0.5778970941027731,\n",
       "   0.5850202429149798,\n",
       "   0.5657569334490169],\n",
       "  'conscientiousness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5809716599190283,\n",
       "   0.5773497866027578,\n",
       "   0.5809716599190283,\n",
       "   0.5751117036398035],\n",
       "  'openness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6194331983805668,\n",
       "   0.6271196947097224,\n",
       "   0.6194331983805668,\n",
       "   0.6173305156425974]},\n",
       " 'svm': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                  ('clf', OneVsRestClassifier(estimator=LinearSVC(dual=True)))]),\n",
       "  0.06882591093117409,\n",
       "  0.5872846807797941,\n",
       "  0.5750577367205543,\n",
       "  0.5797504553762942],\n",
       " 'SVM Multi Model': {'extroversion': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5526315789473685,\n",
       "   0.556838894757555,\n",
       "   0.5526315789473685,\n",
       "   0.5533196658585198],\n",
       "  'neuroticism': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5607287449392713,\n",
       "   0.5663114198946931,\n",
       "   0.5607287449392713,\n",
       "   0.559620707066237],\n",
       "  'agreeableness': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5425101214574899,\n",
       "   0.5412987897493554,\n",
       "   0.5425101214574899,\n",
       "   0.5418251310389365],\n",
       "  'conscientiousness': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5587044534412956,\n",
       "   0.5616203888511163,\n",
       "   0.5587044534412956,\n",
       "   0.5594219718210964],\n",
       "  'openness': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5931174089068826,\n",
       "   0.5957483484360364,\n",
       "   0.5931174089068826,\n",
       "   0.5929089743531673]},\n",
       " 'SVM Multi Model Improved': {'extroversion': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5546558704453441,\n",
       "   0.5525036051325399,\n",
       "   0.5546558704453441,\n",
       "   0.5529994638862699],\n",
       "  'neuroticism': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6093117408906883,\n",
       "   0.6251265686986135,\n",
       "   0.6093117408906883,\n",
       "   0.6041539216681973],\n",
       "  'agreeableness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5748987854251012,\n",
       "   0.5677904536176157,\n",
       "   0.5748987854251012,\n",
       "   0.530713498693179],\n",
       "  'conscientiousness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5809716599190283,\n",
       "   0.5784375316509405,\n",
       "   0.5809716599190283,\n",
       "   0.578442482377853],\n",
       "  'openness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6153846153846154,\n",
       "   0.6210154117455854,\n",
       "   0.6153846153846154,\n",
       "   0.6141216973113737]},\n",
       " 'Random Forest': {'extroversion': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.562753036437247,\n",
       "   0.5578727652823507,\n",
       "   0.562753036437247,\n",
       "   0.5549367279118247],\n",
       "  'neuroticism': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5748987854251012,\n",
       "   0.5787393384647389,\n",
       "   0.5748987854251012,\n",
       "   0.5746897363891658],\n",
       "  'agreeableness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5445344129554656,\n",
       "   0.5268815477608048,\n",
       "   0.5445344129554656,\n",
       "   0.5121255162858346],\n",
       "  'conscientiousness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5647773279352226,\n",
       "   0.5655199855491483,\n",
       "   0.5647773279352226,\n",
       "   0.5650910038708736],\n",
       "  'openness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.6174089068825911,\n",
       "   0.6228568106467882,\n",
       "   0.6174089068825911,\n",
       "   0.616245471979544]},\n",
       " 'Random Forest Improved': {}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert results to dataframe\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "\n",
    "parse, category_name = liwc.load_token_parser('data/LIWC2007_English100131.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': ['funct', 'pronoun', 'ppron', 'i'],\n",
       " 'care': ['verb', 'present', 'affect', 'posemo'],\n",
       " 'about': ['funct', 'adverb', 'preps'],\n",
       " 'you': ['funct', 'pronoun', 'ppron', 'you', 'social'],\n",
       " 'abdomen': ['bio', 'body']}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIWC 2007\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = '''i care about you abdomen'''\n",
    "tokens = word_tokenize(sentences.lower())\n",
    "\n",
    "# List of lists of categories for each token\n",
    "matches = {token : [category for category in parse(token)] for token in tokens}\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that do not appear in the LIWC dictionary are not identified. \n",
    "\n",
    "The LIWC dictionary is not perfect, but it is a good starting point, and it is easy to use.\n",
    "\n",
    "One Solution is to use the LIWC dictionary to identify the words that are not in the dictionary, and then use a different method to identify the words that are not in the dictionary, such as another classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4484"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse every word in LIWC2007_English100131.dic\n",
    "from liwc import read_dic\n",
    "from collections import defaultdict\n",
    "\n",
    "lexicon, category_names = read_dic('data/LIWC2007_English100131.dic')\n",
    "\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store the lexicon (word, category)\n",
    "\n",
    "liwc_lexicon = defaultdict(list)\n",
    "for word, categories in lexicon.items():\n",
    "    liwc_lexicon['word'].append(word_tokenize(word)[0])\n",
    "    liwc_lexicon['categories'].append(categories)\n",
    "\n",
    "liwc_lexicon = pd.DataFrame(liwc_lexicon)\n",
    "\n",
    "liwc_lexicon.head()\n",
    "liwc_lexicon.to_csv('data/liwc_lexicon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>funct</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>achieve</th>\n",
       "      <th>leisure</th>\n",
       "      <th>home</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>filler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdomen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abilit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>able</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  preprocessed_text  funct  pronoun  ppron  i  we  you  shehe  they  ipron  \\\n",
       "0                 a      1        0      0  0   0    0      0     0      0   \n",
       "1           abandon      0        0      0  0   0    0      0     0      0   \n",
       "2           abdomen      0        0      0  0   0    0      0     0      0   \n",
       "3            abilit      0        0      0  0   0    0      0     0      0   \n",
       "4              able      0        0      0  0   0    0      0     0      0   \n",
       "\n",
       "   ...  work  achieve  leisure  home  money  relig  death  assent  nonfl  \\\n",
       "0  ...     0        0        0     0      0      0      0       0      0   \n",
       "1  ...     0        0        0     0      0      0      0       0      0   \n",
       "2  ...     0        0        0     0      0      0      0       0      0   \n",
       "3  ...     0        1        0     0      0      0      0       0      0   \n",
       "4  ...     0        1        0     0      0      0      0       0      0   \n",
       "\n",
       "   filler  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_lexicon = defaultdict(list)\n",
    "for word, categories in lexicon.items():\n",
    "    liwc_lexicon['preprocessed_text'].append(word_tokenize(word)[0])\n",
    "    for category in category_names:\n",
    "        liwc_lexicon[category].append(int(category in categories))\n",
    "\n",
    "liwc_lexicon = pd.DataFrame(liwc_lexicon)\n",
    "\n",
    "liwc_lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC(dual=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for funct\n",
      "funct: [0.9119286510590858, 0.9152712894495652, 0.9119286510590858, 0.8849698203357689]\n",
      "Training model for pronoun\n",
      "pronoun: [0.9821627647714605, 0.9806922306922307, 0.9821627647714605, 0.9783317325882593]\n",
      "Training model for ppron\n",
      "ppron: [0.9899665551839465, 0.9890613067179311, 0.9899665551839465, 0.988459555567306]\n",
      "Training model for i\n",
      "i: [0.9977703455964325, 0.9955456625516245, 0.9977703455964325, 0.9966567626214365]\n",
      "Training model for we\n",
      "we: [0.9977703455964325, 0.9977753225035833, 0.9977703455964325, 0.9972141776125258]\n",
      "Training model for you\n",
      "you: [0.9988851727982163, 0.9988864184152014, 0.9988851727982163, 0.9987740016563502]\n",
      "Training model for shehe\n",
      "shehe: [0.9988851727982163, 0.9988864184152014, 0.9988851727982163, 0.9987740016563502]\n",
      "Training model for they\n",
      "they: [0.9966555183946488, 0.9962864033479731, 0.9966555183946488, 0.9964175626324966]\n",
      "Training model for ipron\n",
      "ipron: [0.992196209587514, 0.9922571767001115, 0.992196209587514, 0.9891767037547089]\n",
      "Training model for article\n",
      "article: [0.9988851727982163, 0.9977715884361225, 0.9988851727982163, 0.9983280700805375]\n",
      "Training model for verb\n",
      "verb: [0.9253065774804905, 0.9217091718263806, 0.9253065774804905, 0.9019246508597493]\n",
      "Training model for auxverb\n",
      "auxverb: [0.9721293199554069, 0.9675266762223285, 0.9721293199554069, 0.9672244821587662]\n",
      "Training model for past\n",
      "past: [0.9721293199554069, 0.9729087013884213, 0.9721293199554069, 0.9610887508564473]\n",
      "Training model for present\n",
      "present: [0.9632107023411371, 0.952927855466519, 0.9632107023411371, 0.9481235194036164]\n",
      "Training model for future\n",
      "future: [0.9866220735785953, 0.9868014424244689, 0.9866220735785953, 0.9816504960291335]\n",
      "Training model for adverb\n",
      "adverb: [0.9810479375696767, 0.9624550558097162, 0.9810479375696767, 0.9716625605642268]\n",
      "Training model for preps\n",
      "preps: [0.9855072463768116, 0.9712245326612057, 0.9855072463768116, 0.978313762826616]\n",
      "Training model for conj\n",
      "conj: [0.9910813823857302, 0.98224230651161, 0.9910813823857302, 0.9866420480861301]\n",
      "Training model for negate\n",
      "negate: [0.9832775919732442, 0.9733788242688155, 0.9832775919732442, 0.978303169079619]\n",
      "Training model for quant\n",
      "quant: [0.9788182831661093, 0.9580852314602497, 0.9788182831661093, 0.9683407916843312]\n",
      "Training model for number\n",
      "number: [0.9933110367892977, 0.9866668158074294, 0.9933110367892977, 0.9899777782765818]\n",
      "Training model for swear\n",
      "swear: [0.9899665551839465, 0.9800337803827698, 0.9899665551839465, 0.9849751271746157]\n",
      "Training model for social\n",
      "social: [0.9141583054626533, 0.9216609280272114, 0.9141583054626533, 0.8857994420006543]\n",
      "Training model for family\n",
      "family: [0.9832775919732442, 0.9835581692891293, 0.9832775919732442, 0.9773760411588706]\n",
      "Training model for friend\n",
      "friend: [0.9933110367892977, 0.9866668158074294, 0.9933110367892977, 0.9899777782765818]\n",
      "Training model for humans\n",
      "humans: [0.9910813823857302, 0.9911611910444709, 0.9910813823857302, 0.9885532477618456]\n",
      "Training model for affect\n",
      "affect: [0.7993311036789298, 0.638930213308576, 0.7993311036789298, 0.7101863709266327]\n",
      "Training model for posemo\n",
      "posemo: [0.919732441471572, 0.8459077638952585, 0.919732441471572, 0.8812767296330393]\n",
      "Training model for negemo\n",
      "negemo: [0.8795986622073578, 0.7736938065569736, 0.8795986622073578, 0.823254263916495]\n",
      "Training model for anx\n",
      "anx: [0.9810479375696767, 0.9624550558097162, 0.9810479375696767, 0.9716625605642268]\n",
      "Training model for anger\n",
      "anger: [0.9498327759197325, 0.9021823022113847, 0.9498327759197325, 0.9253945398326039]\n",
      "Training model for sad\n",
      "sad: [0.9821627647714605, 0.9646436965035191, 0.9821627647714605, 0.9733244046835284]\n",
      "Training model for cogmech\n",
      "cogmech: [0.835005574136009, 0.8623506749510936, 0.835005574136009, 0.7641929481394365]\n",
      "Training model for insight\n",
      "insight: [0.9565217391304348, 0.9149338374291116, 0.9565217391304348, 0.9352657004830918]\n",
      "Training model for cause\n",
      "cause: [0.9665551839464883, 0.9342289236138298, 0.9665551839464883, 0.950117170614065]\n",
      "Training model for discrep\n",
      "discrep: [0.9821627647714605, 0.9824823568920166, 0.9821627647714605, 0.9762976399675]\n",
      "Training model for tentat\n",
      "tentat: [0.9698996655518395, 0.9407053612375701, 0.9698996655518395, 0.9550794669271084]\n",
      "Training model for certain\n",
      "certain: [0.9821627647714605, 0.9824812868291128, 0.9821627647714605, 0.9743154525763221]\n",
      "Training model for inhib\n",
      "inhib: [0.9743589743589743, 0.9493754109138723, 0.9743589743589743, 0.9617049617049617]\n",
      "Training model for incl\n",
      "incl: [0.9955406911928651, 0.9911012678207677, 0.9955406911928651, 0.9933160192572386]\n",
      "Training model for excl\n",
      "excl: [0.9966555183946488, 0.9933222223465061, 0.9966555183946488, 0.9949860786653445]\n",
      "Training model for percept\n",
      "percept: [0.9587513935340022, 0.9604547400063703, 0.9587513935340022, 0.9396195612472089]\n",
      "Training model for see\n",
      "see: [0.9888517279821628, 0.9778277399333093, 0.9888517279821628, 0.9833088371302449]\n",
      "Training model for hear\n",
      "hear: [0.992196209587514, 0.98445331831983, 0.992196209587514, 0.9883095988056938]\n",
      "Training model for feel\n",
      "feel: [0.9899665551839465, 0.9900673375537505, 0.9899665551839465, 0.985887286894383]\n",
      "Training model for bio\n",
      "bio: [0.8807134894091416, 0.8949586419015766, 0.8807134894091416, 0.8259520409732457]\n",
      "Training model for body\n",
      "body: [0.9665551839464883, 0.9676749880554227, 0.9665551839464883, 0.9511626439246978]\n",
      "Training model for health\n",
      "health: [0.9498327759197325, 0.9021823022113847, 0.9498327759197325, 0.9253945398326039]\n",
      "Training model for sexual\n",
      "sexual: [0.9855072463768116, 0.9712245326612057, 0.9855072463768116, 0.978313762826616]\n",
      "Training model for ingest\n",
      "ingest: [0.9721293199554069, 0.9450354147169618, 0.9721293199554069, 0.9583909180340473]\n",
      "Training model for relativ\n",
      "relativ: [0.8483835005574136, 0.8713967192228061, 0.8483835005574136, 0.779899719408226]\n",
      "Training model for motion\n",
      "motion: [0.9531772575250836, 0.9085468842630396, 0.9531772575250836, 0.9303271177898933]\n",
      "Training model for space\n",
      "space: [0.9509476031215162, 0.953356426182513, 0.9509476031215162, 0.928105125398927]\n",
      "Training model for time\n",
      "time: [0.9464882943143813, 0.8958400912741469, 0.9464882943143813, 0.9204679975634704]\n",
      "Training model for work\n",
      "work: [0.9297658862876255, 0.8644646033042136, 0.9297658862876255, 0.8959269198889424]\n",
      "Training model for achieve\n",
      "achieve: [0.9498327759197325, 0.9021823022113847, 0.9498327759197325, 0.9253945398326039]\n",
      "Training model for leisure\n",
      "leisure: [0.955406911928651, 0.9128023673610413, 0.955406911928651, 0.933618840961065]\n",
      "Training model for home\n",
      "home: [0.9765886287625418, 0.9537253498283017, 0.9765886287625418, 0.9650215891663696]\n",
      "Training model for money\n",
      "money: [0.9609810479375697, 0.9234845744951895, 0.9609810479375697, 0.9418597650053269]\n",
      "Training model for relig\n",
      "relig: [0.9620958751393534, 0.9256284729601583, 0.9620958751393534, 0.9435099320968886]\n",
      "Training model for death\n",
      "death: [0.9866220735785953, 0.9734231160725272, 0.9866220735785953, 0.9799781538911974]\n",
      "Training model for assent\n",
      "assent: [0.9899665551839465, 0.9800337803827698, 0.9899665551839465, 0.9849751271746157]\n",
      "Training model for nonfl\n",
      "nonfl: [0.9955406911928651, 0.9911012678207677, 0.9955406911928651, 0.9933160192572386]\n",
      "Training model for filler\n",
      "filler: [0.9988851727982163, 0.9977715884361225, 0.9988851727982163, 0.9983280700805375]\n"
     ]
    }
   ],
   "source": [
    "for i in category_names:\n",
    "    print(f\"{i}: {train_model(liwc_lexicon, i, liwc_model, None)[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to store each category's model\n",
    "liwc_models = [train_model(liwc_lexicon, i, liwc_model, None)[0] for i in category_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_concatenate(models, test_sentence):\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        \n",
    "        # Preprocess the text\n",
    "        test_sentence_processed = preprocess_text(test_sentence)\n",
    "        test_sentence_processed = ' '.join(map(str, test_sentence_processed)) # type: ignore\n",
    "\n",
    "        # Predicting with each model\n",
    "        pred = model.predict([test_sentence_processed])\n",
    "        \n",
    "        # Assuming binary classification, ensuring binary output (0 or 1)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # Transposing to get predictions for each sample across all models\n",
    "    predictions_per_sample = list(zip(*predictions))\n",
    "\n",
    "    # Concatenating binary predictions for each sample into a single string\n",
    "    concatenated_predictions = [''.join(map(str, sample_preds)) for sample_preds in predictions_per_sample]\n",
    "\n",
    "    return concatenated_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0000000000000000000000000000000000000000000000000000000000000000']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_and_concatenate(liwc_models, 'i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11111']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_and_concatenate(svm_models, 'I hate you')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
