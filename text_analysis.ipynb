{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading 15 million reviews \n",
    "\n",
    "*what is the best way to read 15 million reviews?* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(file_name, head=100):\n",
    "    # 15739967\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            \n",
    "            # only get review_text and review_stars\n",
    "            data.append([d['review_text'], d['n_votes'], d['rating']])\n",
    "\n",
    "\n",
    "            # break if reaches the headth line\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews = pd.DataFrame(format_data(os.path.join(\n",
    "#     DIR, \"goodreads_reviews_dedup.json.gz\"))\n",
    "# )\n",
    "\n",
    "reviews = pd.read_csv(\"data/liwc_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>rating</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WC</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Tone</th>\n",
       "      <th>WPS</th>\n",
       "      <th>...</th>\n",
       "      <th>nonflu</th>\n",
       "      <th>filler</th>\n",
       "      <th>AllPunc</th>\n",
       "      <th>Period</th>\n",
       "      <th>Comma</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>OtherP</th>\n",
       "      <th>Emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mind blowingly cool. Best science fiction I've...</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>32.86</td>\n",
       "      <td>27.51</td>\n",
       "      <td>39.59</td>\n",
       "      <td>65.53</td>\n",
       "      <td>19.33</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.79</td>\n",
       "      <td>5.17</td>\n",
       "      <td>2.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.87</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a special book. It started slow for ab...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>358</td>\n",
       "      <td>63.49</td>\n",
       "      <td>35.27</td>\n",
       "      <td>64.13</td>\n",
       "      <td>45.44</td>\n",
       "      <td>17.90</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.80</td>\n",
       "      <td>5.31</td>\n",
       "      <td>3.07</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.84</td>\n",
       "      <td>4.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I haven't read a fun mystery book in a while a...</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>22.00</td>\n",
       "      <td>6.98</td>\n",
       "      <td>91.21</td>\n",
       "      <td>95.15</td>\n",
       "      <td>12.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.28</td>\n",
       "      <td>6.98</td>\n",
       "      <td>2.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.16</td>\n",
       "      <td>4.65</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fun, fast paced, and disturbing tale of murder...</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>23.12</td>\n",
       "      <td>10.38</td>\n",
       "      <td>83.40</td>\n",
       "      <td>27.64</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.64</td>\n",
       "      <td>5.03</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>6.70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fun book that gives you a sense of living in...</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>92.60</td>\n",
       "      <td>32.92</td>\n",
       "      <td>56.26</td>\n",
       "      <td>99.00</td>\n",
       "      <td>15.80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.59</td>\n",
       "      <td>6.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Tessa Bailey is known for writing the dirtiest...</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>39.70</td>\n",
       "      <td>58.73</td>\n",
       "      <td>4.94</td>\n",
       "      <td>75.21</td>\n",
       "      <td>12.75</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.04</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.49</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.78</td>\n",
       "      <td>3.14</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>4.5 stars!! Sweet Filthy Boy is the first book...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>273</td>\n",
       "      <td>88.48</td>\n",
       "      <td>19.55</td>\n",
       "      <td>61.86</td>\n",
       "      <td>85.37</td>\n",
       "      <td>17.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.29</td>\n",
       "      <td>5.13</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.20</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>This is an unexpectedly funny book with lots o...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>41.04</td>\n",
       "      <td>69.35</td>\n",
       "      <td>29.07</td>\n",
       "      <td>99.00</td>\n",
       "      <td>19.89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.29</td>\n",
       "      <td>3.91</td>\n",
       "      <td>3.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.12</td>\n",
       "      <td>2.79</td>\n",
       "      <td>1.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>I liked this installment better than the first...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>32.41</td>\n",
       "      <td>14.43</td>\n",
       "      <td>66.26</td>\n",
       "      <td>84.42</td>\n",
       "      <td>13.29</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.20</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.45</td>\n",
       "      <td>4.30</td>\n",
       "      <td>1.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>This novella gets 5 stars for the absolutely s...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>126</td>\n",
       "      <td>70.33</td>\n",
       "      <td>13.82</td>\n",
       "      <td>29.04</td>\n",
       "      <td>20.23</td>\n",
       "      <td>25.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.32</td>\n",
       "      <td>3.97</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>2.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_text  n_votes  rating  \\\n",
       "0    Mind blowingly cool. Best science fiction I've...       16       5   \n",
       "1    This is a special book. It started slow for ab...       28       5   \n",
       "2    I haven't read a fun mystery book in a while a...        6       3   \n",
       "3    Fun, fast paced, and disturbing tale of murder...       22       4   \n",
       "4    A fun book that gives you a sense of living in...        8       4   \n",
       "..                                                 ...      ...     ...   \n",
       "995  Tessa Bailey is known for writing the dirtiest...        0       5   \n",
       "996  4.5 stars!! Sweet Filthy Boy is the first book...        0       4   \n",
       "997  This is an unexpectedly funny book with lots o...        1       4   \n",
       "998  I liked this installment better than the first...        1       4   \n",
       "999  This novella gets 5 stars for the absolutely s...        5       3   \n",
       "\n",
       "     Segment   WC  Analytic  Clout  Authentic   Tone    WPS  ...  nonflu  \\\n",
       "0          1  174     32.86  27.51      39.59  65.53  19.33  ...     0.0   \n",
       "1          1  358     63.49  35.27      64.13  45.44  17.90  ...     0.0   \n",
       "2          1   86     22.00   6.98      91.21  95.15  12.29  ...     0.0   \n",
       "3          1  179     23.12  10.38      83.40  27.64  19.89  ...     0.0   \n",
       "4          1   79     92.60  32.92      56.26  99.00  15.80  ...     0.0   \n",
       "..       ...  ...       ...    ...        ...    ...    ...  ...     ...   \n",
       "995        1  255     39.70  58.73       4.94  75.21  12.75  ...     0.0   \n",
       "996        1  273     88.48  19.55      61.86  85.37  17.06  ...     0.0   \n",
       "997        1  179     41.04  69.35      29.07  99.00  19.89  ...     0.0   \n",
       "998        1   93     32.41  14.43      66.26  84.42  13.29  ...     0.0   \n",
       "999        1  126     70.33  13.82      29.04  20.23  25.20  ...     0.0   \n",
       "\n",
       "     filler  AllPunc  Period  Comma  QMark  Exclam  Apostro  OtherP  Emoji  \n",
       "0       0.0    13.79    5.17   2.30   0.00    0.00     2.87    3.45      0  \n",
       "1       0.0    14.80    5.31   3.07   1.12    0.00     0.84    4.47      0  \n",
       "2       0.0    16.28    6.98   2.33   0.00    1.16     4.65    1.16      0  \n",
       "3       0.0    15.64    5.03   2.79   0.00    0.00     1.12    6.70      0  \n",
       "4       0.0     7.59    6.33   0.00   0.00    0.00     0.00    1.27      0  \n",
       "..      ...      ...     ...    ...    ...     ...      ...     ...    ...  \n",
       "995     0.0    18.04    5.88   5.49   1.18    0.78     3.14    1.57      0  \n",
       "996     0.0    14.29    5.13   4.03   0.00    2.20     1.47    1.47      0  \n",
       "997     0.0    12.29    3.91   3.35   0.00    1.12     2.79    1.12      0  \n",
       "998     0.0    17.20    4.30   1.08   0.00    6.45     4.30    1.08      0  \n",
       "999     0.0    10.32    3.97   3.17   0.00    0.00     0.79    2.38      0  \n",
       "\n",
       "[1000 rows x 122 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.to_csv(os.path.join(DIR, \"sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(data=reviews, x='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A wonderful, beautifully written, poignant, subtle, and brilliant novel. \n",
      " This is the story of an English butler of one the great houses, whose self value comes from his duty and his dignity, and who questions his values and his life in the end. It is ultimately a story of regret. It is a story that makes you want to live your life and dedicate yourself to things that you won't regret when you look back. One of the best ways of thinking about regret I've ever come across is from Jeff Bezos in his nerdily titled Regret Minimization Framework. \n",
      " But what I really loved about the novel is how slow and forcefully it builds. I don't think I've read a book that does that so well since Pride and Prejudice. In the beginning you think it's just a butler going on a motoring trip. And at the end, your heart is breaking. \n",
      " Dignity is a big theme in the novel. Stevens attempts to define dignity several times throughout the novel - his definition generally has to do with self-respect and keeping ones emotions in check. But you have to wonder - his is suppression of emotions what leads him to where he ends up? His pushing himself to \"banter\" is a an interesting indication of him trying to break that pattern. \n",
      " \"We may now understand better, too, why my father was so fond of the story of the butler who failed to panic on discovering a tiger under the dining table; it was because he knew instinctively that somewhere in this story lay the kernel of what true 'dignity' is.\" \n",
      " \"What do you think dignity's all about?' The directness of the inquiry did, I admit, take me rather by surprise. 'It's rather a hard thing to explain in a few words, sir,' I said. 'But I suspect it comes down to not removing one's clothing in public.\" \n",
      " In the end, Stevens breaks your heart in two ways: he missed out on the love of his life, AND he realizes that he dedicated his life to serving a man who in the end wasn't the great man he thought he was. Good reminder: prioritize love first. And work on things that matter. \n",
      " Another aspect I loved is just learning about the era and how things in England worked. Fascinating for instance that multi-day stays at large estates were the only way to get a bunch of people together to communicate on an issue. Today, we communicate in very different ways. Seems appealing to try that way. \n",
      " I'll conclude this with another of my favorite quotes from the novel: \n",
      " You've got to enjoy yourself. The evening's the best part of the day. You've done your day's work. Now you can put your feet up and enjoy it. That's how I look at it. Ask anybody, they'll all tell you. The evening's the best part of the day.\n"
     ]
    }
   ],
   "source": [
    "example = reviews.iloc[55]['review_text']\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A wonderful, beautifully written, poignant, subtle, and brilliant novel.',\n",
       " 'This is the story of an English butler of one the great houses, whose self value comes from his duty and his dignity, and who questions his values and his life in the end.',\n",
       " 'It is ultimately a story of regret.',\n",
       " \"It is a story that makes you want to live your life and dedicate yourself to things that you won't regret when you look back.\",\n",
       " \"One of the best ways of thinking about regret I've ever come across is from Jeff Bezos in his nerdily titled Regret Minimization Framework.\",\n",
       " 'But what I really loved about the novel is how slow and forcefully it builds.',\n",
       " \"I don't think I've read a book that does that so well since Pride and Prejudice.\",\n",
       " \"In the beginning you think it's just a butler going on a motoring trip.\",\n",
       " 'And at the end, your heart is breaking.',\n",
       " 'Dignity is a big theme in the novel.',\n",
       " 'Stevens attempts to define dignity several times throughout the novel - his definition generally has to do with self-respect and keeping ones emotions in check.',\n",
       " 'But you have to wonder - his is suppression of emotions what leads him to where he ends up?',\n",
       " 'His pushing himself to \"banter\" is a an interesting indication of him trying to break that pattern.',\n",
       " '\"We may now understand better, too, why my father was so fond of the story of the butler who failed to panic on discovering a tiger under the dining table; it was because he knew instinctively that somewhere in this story lay the kernel of what true \\'dignity\\' is.\"',\n",
       " '\"What do you think dignity\\'s all about?\\'',\n",
       " 'The directness of the inquiry did, I admit, take me rather by surprise.',\n",
       " \"'It's rather a hard thing to explain in a few words, sir,' I said.\",\n",
       " '\\'But I suspect it comes down to not removing one\\'s clothing in public.\"',\n",
       " \"In the end, Stevens breaks your heart in two ways: he missed out on the love of his life, AND he realizes that he dedicated his life to serving a man who in the end wasn't the great man he thought he was.\",\n",
       " 'Good reminder: prioritize love first.',\n",
       " 'And work on things that matter.',\n",
       " 'Another aspect I loved is just learning about the era and how things in England worked.',\n",
       " 'Fascinating for instance that multi-day stays at large estates were the only way to get a bunch of people together to communicate on an issue.',\n",
       " 'Today, we communicate in very different ways.',\n",
       " 'Seems appealing to try that way.',\n",
       " \"I'll conclude this with another of my favorite quotes from the novel: \\n You've got to enjoy yourself.\",\n",
       " \"The evening's the best part of the day.\",\n",
       " \"You've done your day's work.\",\n",
       " 'Now you can put your feet up and enjoy it.',\n",
       " \"That's how I look at it.\",\n",
       " \"Ask anybody, they'll all tell you.\",\n",
       " \"The evening's the best part of the day.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = sent_tokenize(example)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_terms(sentence):\n",
    "    # Tokenize words and tag part of speech\n",
    "    words = word_tokenize(sentence)\n",
    "    tagged_words = pos_tag(words)\n",
    "    sentiment_terms = set()\n",
    "\n",
    "    for word, tag in tagged_words:\n",
    "        if word.lower() not in stop_words and word not in string.punctuation:\n",
    "            if tag.startswith('JJ') or tag.startswith('RB'):\n",
    "                if word.lower() in opinion_lexicon.positive() or word.lower() in opinion_lexicon.negative():\n",
    "                    sentiment_terms.add(word)\n",
    "\n",
    "    return sentiment_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_terms = set()\n",
    "for sentence in sentences:\n",
    "    sentiment_terms.update(extract_sentiment_terms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good', 'self-respect', 'well', 'better', 'poignant']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_terms = list(sentiment_terms)\n",
    "sentiment_terms[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m sent_tokenize(review)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 6\u001b[0m     sentiment_terms\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mextract_sentiment_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m, in \u001b[0;36mextract_sentiment_terms\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_sentiment_terms\u001b[39m(sentence):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Tokenize words and tag part of speech\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n\u001b[0;32m----> 4\u001b[0m     tagged_words \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     sentiment_terms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, tag \u001b[38;5;129;01min\u001b[39;00m tagged_words:\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/tag/__init__.py:160\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/tag/__init__.py:106\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    104\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/tag/perceptron.py:159\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    156\u001b[0m START \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-START-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-START2-\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    157\u001b[0m END \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-END-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-END2-\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, load\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    160\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m    :param load: Load the pickled model upon instantiation.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m AveragedPerceptron()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get all sentiement terms from all reviews\n",
    "sentiment_terms = set()\n",
    "for review in reviews['review_text']:\n",
    "    sentences = sent_tokenize(review)\n",
    "    for sentence in sentences:\n",
    "        sentiment_terms.update(extract_sentiment_terms(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_word_embeddings(processed_corpus):\n",
    "    # Train a Word2Vec model on the processed corpus\n",
    "    model = Word2Vec(sentences=processed_corpus,\n",
    "                     vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazing: [('sufficient', 0.2987786829471588), ('pretty', 0.2982717454433441), ('creative', 0.2408866584300995), ('ambitious', 0.24001356959342957), ('Well', 0.23229120671749115), ('bullshit', 0.2235596776008606), ('available', 0.2103513479232788), ('valuable', 0.20808722078800201), ('corrupt', 0.19869357347488403), ('hilarious', 0.19359393417835236)]\n"
     ]
    }
   ],
   "source": [
    "model = learn_word_embeddings([word_tokenize(word) for word in sentiment_terms])\n",
    "\n",
    "token = 'amazing'\n",
    "\n",
    "if token in model.wv:\n",
    "    print(f\"{token}: {model.wv.most_similar(token)}\")\n",
    "else:\n",
    "    synonyms = get_synonyms(token)\n",
    "\n",
    "    for synonym in synonyms:\n",
    "        if synonym in model.wv:\n",
    "            print(f\"{synonym}: {model.wv.most_similar(synonym)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays = pd.read_csv('data/essays.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/qth2myc91gj4tfy56qzfn3b40000gn/T/ipykernel_97856/1694335583.py:15: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  essays = essays.replace({'n': 0, 'y': 1})\n"
     ]
    }
   ],
   "source": [
    "essays = essays.rename(columns={\n",
    "    'TEXT': 'text',\n",
    "    'cEXT': 'extroversion',\n",
    "    'cNEU': 'neuroticism',\n",
    "    'cAGR': 'agreeableness',\n",
    "    'cCON': 'conscientiousness',\n",
    "    'cOPN': 'openness'\n",
    "})\n",
    "\n",
    "# subset the data text + big five\n",
    "\n",
    "essays = essays[['text', 'extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']]\n",
    "\n",
    "# convert n and y to 0 and 1\n",
    "essays = essays.replace({'n': 0, 'y': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extroversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  extroversion  \\\n",
       "0     Well, right now I just woke up from a mid-day ...             0   \n",
       "1     Well, here we go with the stream of consciousn...             0   \n",
       "2     An open keyboard and buttons to push. The thin...             0   \n",
       "3     I can't believe it!  It's really happening!  M...             1   \n",
       "4     Well, here I go with the good old stream of co...             1   \n",
       "...                                                 ...           ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...             0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...             1   \n",
       "2464  It is Wednesday, December 8th and a lot has be...             0   \n",
       "2465  Man this week has been hellish. Anyways, now i...             0   \n",
       "2466  I have just gotten off the phone with brady. I...             0   \n",
       "\n",
       "      neuroticism  agreeableness  conscientiousness  openness  \n",
       "0               1              1                  0         1  \n",
       "1               0              1                  0         0  \n",
       "2               1              0                  1         1  \n",
       "3               0              1                  1         0  \n",
       "4               0              1                  0         1  \n",
       "...           ...            ...                ...       ...  \n",
       "2462            1              0                  1         0  \n",
       "2463            1              0                  0         1  \n",
       "2464            0              1                  0         0  \n",
       "2465            1              0                  0         1  \n",
       "2466            1              1                  0         1  \n",
       "\n",
       "[2467 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "essays.to_pickle('data/essays.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jessedoka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from preprocessing import preprocess_text\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "def load_data(file_path):\n",
    "    # Load Pickled data\n",
    "    df = pd.read_pickle(file_path)\n",
    "\n",
    "    df['preprocessed_text'] = df['text'].progress_apply(lambda x: ' '.join(map(str, preprocess_text(x)))) # type: ignore\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred):\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, target_columns, model, model_path):\n",
    "\n",
    "    X = df['preprocessed_text']\n",
    "    y = df[target_columns]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy, precision, recall, f1 = evaluate_model(y_test, y_pred)\n",
    "\n",
    "    if model_path is not None:\n",
    "        joblib.dump(model, model_path)\n",
    "\n",
    "    return [model, accuracy, precision, recall, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2467 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 21/2467 [00:07<13:57,  2.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/essays.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load Pickled data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/tqdm/std.py:917\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;66;03m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m     t\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/series.py:4915\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4780\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4781\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4782\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4906\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4907\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4909\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4913\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/tqdm/std.py:912\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;66;03m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    911\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mn \u001b[38;5;241m<\u001b[39m t\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 912\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m, in \u001b[0;36mload_data.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(file_path):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Load Pickled data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(file_path)\n\u001b[0;32m----> 7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mprogress_apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))) \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/dev/LCT/preprocessing.py:88\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     85\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_tokenize(sentence)])\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Extract sentiment terms\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m sentiment_terms \u001b[38;5;241m=\u001b[39m \u001b[43mextract_sentiment_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Tokenize words\u001b[39;00m\n\u001b[1;32m     91\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m word_tokenize(sentence)\n",
      "File \u001b[0;32m~/dev/LCT/preprocessing.py:37\u001b[0m, in \u001b[0;36mextract_sentiment_terms\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string\u001b[38;5;241m.\u001b[39mpunctuation:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJJ\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRB\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 37\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m opinion_lexicon\u001b[38;5;241m.\u001b[39mnegative() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopinion_lexicon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m                 sentiment_terms\u001b[38;5;241m.\u001b[39madd(word)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sentiment_terms\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:199\u001b[0m, in \u001b[0;36mAbstractLazySequence.__contains__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__contains__\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return true if this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:184\u001b[0m, in \u001b[0;36mAbstractLazySequence.count\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m elt \u001b[38;5;241m==\u001b[39m value)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/collections.py:184\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m    183\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the number of times this list contains ``value``.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/corpus/reader/util.py:293\u001b[0m, in \u001b[0;36mStreamBackedCorpusView.iterate_from\u001b[0;34m(self, start_tok)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Each iteration through this loop, we read a single block\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# from the stream.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m filepos \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eofpos:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m# Read the next block.\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_toknum \u001b[38;5;241m=\u001b[39m toknum\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_blocknum \u001b[38;5;241m=\u001b[39m block_index\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/nltk/data.py:1215\u001b[0m, in \u001b[0;36mSeekableUnicodeStreamReader.seek\u001b[0;34m(self, offset, whence)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m whence \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1211\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRelative seek is not supported for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1212\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeekableUnicodeStreamReader -- consider \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1213\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing char_seek_forward() instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m     )\n\u001b[0;32m-> 1215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mseek(offset, whence)\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinebuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbytebuffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = load_data('data/essays.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/essays_preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>extroversion</th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>agreeableness</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>openness</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Well, right now I just woke up from a mid-day ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', ',', 'right', 'woke', 'mid-day', 'nap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well, here we go with the stream of consciousn...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['well', ',', 'go', 'stream', 'consciousness',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An open keyboard and buttons to push. The thin...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['open', 'keyboard', 'buttons', 'push', '.', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I can't believe it!  It's really happening!  M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['ca', \"n't\", 'believe', '!', \"'s\", 'really', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Well, here I go with the good old stream of co...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['well', ',', 'go', 'good', 'old', 'stream', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2462</th>\n",
       "      <td>I'm home. wanted to go to bed but remembe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[\"'m\", 'home', '.', 'wanted', 'go', 'bed', 're...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Stream of consiousnesssskdj. How do you s...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['stream', 'consiousnesssskdj', '.', 'spell', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2464</th>\n",
       "      <td>It is Wednesday, December 8th and a lot has be...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['wednesday', ',', 'december', '8th', 'lot', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>Man this week has been hellish. Anyways, now i...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['man', 'week', 'hellish', '.', 'anyways', ','...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>I have just gotten off the phone with brady. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>['gotten', 'phone', 'brady', '.', \"'m\", 'tryin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2467 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  extroversion  \\\n",
       "0     Well, right now I just woke up from a mid-day ...             0   \n",
       "1     Well, here we go with the stream of consciousn...             0   \n",
       "2     An open keyboard and buttons to push. The thin...             0   \n",
       "3     I can't believe it!  It's really happening!  M...             1   \n",
       "4     Well, here I go with the good old stream of co...             1   \n",
       "...                                                 ...           ...   \n",
       "2462       I'm home. wanted to go to bed but remembe...             0   \n",
       "2463       Stream of consiousnesssskdj. How do you s...             1   \n",
       "2464  It is Wednesday, December 8th and a lot has be...             0   \n",
       "2465  Man this week has been hellish. Anyways, now i...             0   \n",
       "2466  I have just gotten off the phone with brady. I...             0   \n",
       "\n",
       "      neuroticism  agreeableness  conscientiousness  openness  \\\n",
       "0               1              1                  0         1   \n",
       "1               0              1                  0         0   \n",
       "2               1              0                  1         1   \n",
       "3               0              1                  1         0   \n",
       "4               0              1                  0         1   \n",
       "...           ...            ...                ...       ...   \n",
       "2462            1              0                  1         0   \n",
       "2463            1              0                  0         1   \n",
       "2464            0              1                  0         0   \n",
       "2465            1              0                  0         1   \n",
       "2466            1              1                  0         1   \n",
       "\n",
       "                                      preprocessed_text  \n",
       "0     ['well', ',', 'right', 'woke', 'mid-day', 'nap...  \n",
       "1     ['well', ',', 'go', 'stream', 'consciousness',...  \n",
       "2     ['open', 'keyboard', 'buttons', 'push', '.', '...  \n",
       "3     ['ca', \"n't\", 'believe', '!', \"'s\", 'really', ...  \n",
       "4     ['well', ',', 'go', 'good', 'old', 'stream', '...  \n",
       "...                                                 ...  \n",
       "2462  [\"'m\", 'home', '.', 'wanted', 'go', 'bed', 're...  \n",
       "2463  ['stream', 'consiousnesssskdj', '.', 'spell', ...  \n",
       "2464  ['wednesday', ',', 'december', '8th', 'lot', '...  \n",
       "2465  ['man', 'week', 'hellish', '.', 'anyways', ','...  \n",
       "2466  ['gotten', 'phone', 'brady', '.', \"'m\", 'tryin...  \n",
       "\n",
       "[2467 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "traits = ['extroversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"logistic_regression\"] = train_model(df, traits, model, 'models/big_five_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__estimator__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "results[\"logistic_regression improved\"] = train_model(df, traits, grid_search, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Model Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Training model for neuroticism\n",
      "Training model for agreeableness\n",
      "Training model for conscientiousness\n",
      "Training model for openness\n"
     ]
    }
   ],
   "source": [
    "results[\"LR Multi-Model\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"LR Multi-Model\"][t] = train_model(df, t, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for neuroticism\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for agreeableness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for conscientiousness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for openness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "results[\"LR Multi-Model Classification Improved\"] = {}\n",
    "\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"LR Multi-Model Classification Improved\"][t] = train_model(df, t, grid_search, None)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(dual=True)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"svm\"] = train_model(df, traits, model, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OneVsRestClassifier is a simple way to train a multi-label classification model.\n",
    "\n",
    "But its performance is not good enough. with accuracy of 0.07. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC(dual=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Training model for neuroticism\n",
      "Training model for agreeableness\n",
      "Training model for conscientiousness\n",
      "Training model for openness\n"
     ]
    }
   ],
   "source": [
    "results[\"SVM Multi Model\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"SVM Multi Model\"][t] = train_model(df, t, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for neuroticism\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for agreeableness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for conscientiousness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Training model for openness\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "results[\"SVM Multi Model Improved\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"SVM Multi Model Improved\"][t] = train_model(df, t, grid_search, f'models/{t}_model_cv.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randmom Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for neuroticism\n",
      "Training model for agreeableness\n",
      "Training model for conscientiousness\n",
      "Training model for openness\n"
     ]
    }
   ],
   "source": [
    "results[\"Random Forest\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"Random Forest\"][t] = train_model(df, t, model, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for extroversion\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for neuroticism\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Training model for agreeableness\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Training model for conscientiousness\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n",
      "Training model for openness\n",
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'clf__n_estimators': [50, 100, 200],\n",
    "    'clf__max_depth': [10, 20, 30]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, parameters, n_jobs=-1, verbose=1)\n",
    "results[\"Random Forest Improved\"] = {}\n",
    "for t in traits:\n",
    "    print(f'Training model for {t}')\n",
    "    results[\"Random Forest Improved\"][t] = train_model(df, t, grid_search, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logistic_regression': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                  ('clf',\n",
       "                   OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000)))]),\n",
       "  0.0728744939271255,\n",
       "  0.5926256141521749,\n",
       "  0.625866050808314,\n",
       "  0.6079697557341297],\n",
       " 'logistic_regression improved': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                         ('clf',\n",
       "                                          OneVsRestClassifier(estimator=LogisticRegression(max_iter=1000)))]),\n",
       "               n_jobs=-1,\n",
       "               param_grid={'clf__estimator__C': [0.1, 1, 10],\n",
       "                           'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                           'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "               verbose=1),\n",
       "  0.0728744939271255,\n",
       "  0.5953794656683132,\n",
       "  0.6289453425712086,\n",
       "  0.6106791071582232],\n",
       " 'LR Multi-Model': {'extroversion': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5566801619433198,\n",
       "   0.5554302204838598,\n",
       "   0.5566801619433198,\n",
       "   0.5558710736548559],\n",
       "  'neuroticism': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5910931174089069,\n",
       "   0.5958157512391467,\n",
       "   0.5910931174089069,\n",
       "   0.5906372344712552],\n",
       "  'agreeableness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5647773279352226,\n",
       "   0.557906411630976,\n",
       "   0.5647773279352226,\n",
       "   0.5570403080475619],\n",
       "  'conscientiousness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.5587044534412956,\n",
       "   0.5599262454805568,\n",
       "   0.5587044534412956,\n",
       "   0.5591693344344936],\n",
       "  'openness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', LogisticRegression(max_iter=1000))]),\n",
       "   0.6093117408906883,\n",
       "   0.6095201472791609,\n",
       "   0.6093117408906883,\n",
       "   0.6093934859017636]},\n",
       " 'LR Multi-Model Classification Improved': {'extroversion': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5607287449392713,\n",
       "   0.5597579044083079,\n",
       "   0.5607287449392713,\n",
       "   0.5601311266298393],\n",
       "  'neuroticism': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6093117408906883,\n",
       "   0.6221957755487011,\n",
       "   0.6093117408906883,\n",
       "   0.6055588847875354],\n",
       "  'agreeableness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5850202429149798,\n",
       "   0.5778970941027731,\n",
       "   0.5850202429149798,\n",
       "   0.5657569334490169],\n",
       "  'conscientiousness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5809716599190283,\n",
       "   0.5773497866027578,\n",
       "   0.5809716599190283,\n",
       "   0.5751117036398035],\n",
       "  'openness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf',\n",
       "                                           LogisticRegression(max_iter=1000))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6194331983805668,\n",
       "   0.6271196947097224,\n",
       "   0.6194331983805668,\n",
       "   0.6173305156425974]},\n",
       " 'svm': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                  ('clf', OneVsRestClassifier(estimator=LinearSVC(dual=True)))]),\n",
       "  0.06882591093117409,\n",
       "  0.5872846807797941,\n",
       "  0.5750577367205543,\n",
       "  0.5797504553762942],\n",
       " 'SVM Multi Model': {'extroversion': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5526315789473685,\n",
       "   0.556838894757555,\n",
       "   0.5526315789473685,\n",
       "   0.5533196658585198],\n",
       "  'neuroticism': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5607287449392713,\n",
       "   0.5663114198946931,\n",
       "   0.5607287449392713,\n",
       "   0.559620707066237],\n",
       "  'agreeableness': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5425101214574899,\n",
       "   0.5412987897493554,\n",
       "   0.5425101214574899,\n",
       "   0.5418251310389365],\n",
       "  'conscientiousness': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5587044534412956,\n",
       "   0.5616203888511163,\n",
       "   0.5587044534412956,\n",
       "   0.5594219718210964],\n",
       "  'openness': [Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', LinearSVC(dual=True))]),\n",
       "   0.5931174089068826,\n",
       "   0.5957483484360364,\n",
       "   0.5931174089068826,\n",
       "   0.5929089743531673]},\n",
       " 'SVM Multi Model Improved': {'extroversion': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5546558704453441,\n",
       "   0.5525036051325399,\n",
       "   0.5546558704453441,\n",
       "   0.5529994638862699],\n",
       "  'neuroticism': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6093117408906883,\n",
       "   0.6251265686986135,\n",
       "   0.6093117408906883,\n",
       "   0.6041539216681973],\n",
       "  'agreeableness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5748987854251012,\n",
       "   0.5677904536176157,\n",
       "   0.5748987854251012,\n",
       "   0.530713498693179],\n",
       "  'conscientiousness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5809716599190283,\n",
       "   0.5784375316509405,\n",
       "   0.5809716599190283,\n",
       "   0.578442482377853],\n",
       "  'openness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', LinearSVC(dual=True))]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__C': [0.1, 1, 10],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.6153846153846154,\n",
       "   0.6210154117455854,\n",
       "   0.6153846153846154,\n",
       "   0.6141216973113737]},\n",
       " 'Random Forest': {'extroversion': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5121457489878543,\n",
       "   0.5061834114638823,\n",
       "   0.5121457489878543,\n",
       "   0.5063084709526281],\n",
       "  'neuroticism': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5668016194331984,\n",
       "   0.5676104862707734,\n",
       "   0.5668016194331984,\n",
       "   0.5670575796107916],\n",
       "  'agreeableness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5344129554655871,\n",
       "   0.5237885480898612,\n",
       "   0.5344129554655871,\n",
       "   0.5227277969967323],\n",
       "  'conscientiousness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.5080971659919028,\n",
       "   0.5098894188242099,\n",
       "   0.5080971659919028,\n",
       "   0.5087423972016258],\n",
       "  'openness': [Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                   ('clf', RandomForestClassifier())]),\n",
       "   0.6133603238866396,\n",
       "   0.6187321249489285,\n",
       "   0.6133603238866396,\n",
       "   0.612184577503137]},\n",
       " 'Random Forest Improved': {'extroversion': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', RandomForestClassifier())]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__max_depth': [10, 20, 30],\n",
       "                            'clf__n_estimators': [50, 100, 200],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5384615384615384,\n",
       "   0.5361530603437726,\n",
       "   0.5384615384615384,\n",
       "   0.5367448989366798],\n",
       "  'neuroticism': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', RandomForestClassifier())]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__max_depth': [10, 20, 30],\n",
       "                            'clf__n_estimators': [50, 100, 200],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5465587044534413,\n",
       "   0.5538277511961723,\n",
       "   0.5465587044534413,\n",
       "   0.5440395861448494],\n",
       "  'agreeableness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', RandomForestClassifier())]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__max_depth': [10, 20, 30],\n",
       "                            'clf__n_estimators': [50, 100, 200],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5931174089068826,\n",
       "   0.6047275339072852,\n",
       "   0.5931174089068826,\n",
       "   0.5356879219501878],\n",
       "  'conscientiousness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', RandomForestClassifier())]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__max_depth': [10, 20, 30],\n",
       "                            'clf__n_estimators': [50, 100, 200],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5445344129554656,\n",
       "   0.5413911495074087,\n",
       "   0.5445344129554656,\n",
       "   0.541785306932449],\n",
       "  'openness': [GridSearchCV(estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                          ('clf', RandomForestClassifier())]),\n",
       "                n_jobs=-1,\n",
       "                param_grid={'clf__max_depth': [10, 20, 30],\n",
       "                            'clf__n_estimators': [50, 100, 200],\n",
       "                            'tfidf__max_df': (0.25, 0.5, 0.75),\n",
       "                            'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "                verbose=1),\n",
       "   0.5809716599190283,\n",
       "   0.5838070682178234,\n",
       "   0.5809716599190283,\n",
       "   0.5806470673945279]}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert results to dataframe\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abdomen'] set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"abdomen\"\n",
    "\n",
    "# Load the model\n",
    "model = joblib.load(f'models/liwc_model.pkl')\n",
    "\n",
    "# Preprocess the text\n",
    "test_sentence_processed = preprocess_text(test_sentence)\n",
    "test_sentence_processed = ' '.join(map(str, test_sentence_processed)) # type: ignore\n",
    "\n",
    "print(test_sentence_processed)\n",
    "# Predict the label\n",
    "\n",
    "prediction = model.predict([test_sentence_processed])\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import liwc\n",
    "\n",
    "parse, category_name = liwc.load_token_parser('data/LIWC2007_English100131.dic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': ['funct', 'pronoun', 'ppron', 'i'],\n",
       " 'care': ['verb', 'present', 'affect', 'posemo'],\n",
       " 'about': ['funct', 'adverb', 'preps'],\n",
       " 'you': ['funct', 'pronoun', 'ppron', 'you', 'social'],\n",
       " 'abdomen': ['bio', 'body']}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIWC 2007\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = '''i care about you abdomen'''\n",
    "tokens = word_tokenize(sentences.lower())\n",
    "\n",
    "# List of lists of categories for each token\n",
    "matches = {token : [category for category in parse(token)] for token in tokens}\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that do not appear in the LIWC dictionary are not identified. \n",
    "\n",
    "The LIWC dictionary is not perfect, but it is a good starting point, and it is easy to use.\n",
    "\n",
    "One Solution is to use the LIWC dictionary to identify the words that are not in the dictionary, and then use a different method to identify the words that are not in the dictionary, such as another classification model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4484"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse every word in LIWC2007_English100131.dic\n",
    "from liwc import read_dic\n",
    "from collections import defaultdict\n",
    "\n",
    "lexicon, category_names = read_dic('data/LIWC2007_English100131.dic')\n",
    "\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to store the lexicon (word, category)\n",
    "\n",
    "liwc_lexicon = defaultdict(list)\n",
    "for word, categories in lexicon.items():\n",
    "    liwc_lexicon['word'].append(word_tokenize(word)[0])\n",
    "    liwc_lexicon['categories'].append(categories)\n",
    "\n",
    "liwc_lexicon = pd.DataFrame(liwc_lexicon)\n",
    "\n",
    "liwc_lexicon.head()\n",
    "liwc_lexicon.to_csv('data/liwc_lexicon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>funct</th>\n",
       "      <th>pronoun</th>\n",
       "      <th>ppron</th>\n",
       "      <th>i</th>\n",
       "      <th>we</th>\n",
       "      <th>you</th>\n",
       "      <th>shehe</th>\n",
       "      <th>they</th>\n",
       "      <th>ipron</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>achieve</th>\n",
       "      <th>leisure</th>\n",
       "      <th>home</th>\n",
       "      <th>money</th>\n",
       "      <th>relig</th>\n",
       "      <th>death</th>\n",
       "      <th>assent</th>\n",
       "      <th>nonfl</th>\n",
       "      <th>filler</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdomen</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abilit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>able</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  preprocessed_text  funct  pronoun  ppron  i  we  you  shehe  they  ipron  \\\n",
       "0                 a      1        0      0  0   0    0      0     0      0   \n",
       "1           abandon      0        0      0  0   0    0      0     0      0   \n",
       "2           abdomen      0        0      0  0   0    0      0     0      0   \n",
       "3            abilit      0        0      0  0   0    0      0     0      0   \n",
       "4              able      0        0      0  0   0    0      0     0      0   \n",
       "\n",
       "   ...  work  achieve  leisure  home  money  relig  death  assent  nonfl  \\\n",
       "0  ...     0        0        0     0      0      0      0       0      0   \n",
       "1  ...     0        0        0     0      0      0      0       0      0   \n",
       "2  ...     0        0        0     0      0      0      0       0      0   \n",
       "3  ...     0        1        0     0      0      0      0       0      0   \n",
       "4  ...     0        1        0     0      0      0      0       0      0   \n",
       "\n",
       "   filler  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liwc_lexicon = defaultdict(list)\n",
    "for word, categories in lexicon.items():\n",
    "    liwc_lexicon['preprocessed_text'].append(word_tokenize(word)[0])\n",
    "    for category in category_names:\n",
    "        liwc_lexicon[category].append(int(category in categories))\n",
    "\n",
    "liwc_lexicon = pd.DataFrame(liwc_lexicon)\n",
    "\n",
    "liwc_lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_model = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC(dual=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for funct\n",
      "funct: [0.9119286510590858, 0.9152712894495652, 0.9119286510590858, 0.8849698203357689]\n",
      "Training model for pronoun\n",
      "pronoun: [0.9821627647714605, 0.9806922306922307, 0.9821627647714605, 0.9783317325882593]\n",
      "Training model for ppron\n",
      "ppron: [0.9899665551839465, 0.9890613067179311, 0.9899665551839465, 0.988459555567306]\n",
      "Training model for i\n",
      "i: [0.9977703455964325, 0.9955456625516245, 0.9977703455964325, 0.9966567626214365]\n",
      "Training model for we\n",
      "we: [0.9977703455964325, 0.9977753225035833, 0.9977703455964325, 0.9972141776125258]\n",
      "Training model for you\n",
      "you: [0.9988851727982163, 0.9988864184152014, 0.9988851727982163, 0.9987740016563502]\n",
      "Training model for shehe\n",
      "shehe: [0.9988851727982163, 0.9988864184152014, 0.9988851727982163, 0.9987740016563502]\n",
      "Training model for they\n",
      "they: [0.9966555183946488, 0.9962864033479731, 0.9966555183946488, 0.9964175626324966]\n",
      "Training model for ipron\n",
      "ipron: [0.992196209587514, 0.9922571767001115, 0.992196209587514, 0.9891767037547089]\n",
      "Training model for article\n",
      "article: [0.9988851727982163, 0.9977715884361225, 0.9988851727982163, 0.9983280700805375]\n",
      "Training model for verb\n",
      "verb: [0.9253065774804905, 0.9217091718263806, 0.9253065774804905, 0.9019246508597493]\n",
      "Training model for auxverb\n",
      "auxverb: [0.9721293199554069, 0.9675266762223285, 0.9721293199554069, 0.9672244821587662]\n",
      "Training model for past\n",
      "past: [0.9721293199554069, 0.9729087013884213, 0.9721293199554069, 0.9610887508564473]\n",
      "Training model for present\n",
      "present: [0.9632107023411371, 0.952927855466519, 0.9632107023411371, 0.9481235194036164]\n",
      "Training model for future\n",
      "future: [0.9866220735785953, 0.9868014424244689, 0.9866220735785953, 0.9816504960291335]\n",
      "Training model for adverb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adverb: [0.9810479375696767, 0.9624550558097162, 0.9810479375696767, 0.9716625605642268]\n",
      "Training model for preps\n",
      "preps: [0.9855072463768116, 0.9712245326612057, 0.9855072463768116, 0.978313762826616]\n",
      "Training model for conj\n",
      "conj: [0.9910813823857302, 0.98224230651161, 0.9910813823857302, 0.9866420480861301]\n",
      "Training model for negate\n",
      "negate: [0.9832775919732442, 0.9733788242688155, 0.9832775919732442, 0.978303169079619]\n",
      "Training model for quant\n",
      "quant: [0.9788182831661093, 0.9580852314602497, 0.9788182831661093, 0.9683407916843312]\n",
      "Training model for number\n",
      "number: [0.9933110367892977, 0.9866668158074294, 0.9933110367892977, 0.9899777782765818]\n",
      "Training model for swear\n",
      "swear: [0.9899665551839465, 0.9800337803827698, 0.9899665551839465, 0.9849751271746157]\n",
      "Training model for social\n",
      "social: [0.9141583054626533, 0.9216609280272114, 0.9141583054626533, 0.8857994420006543]\n",
      "Training model for family\n",
      "family: [0.9832775919732442, 0.9835581692891293, 0.9832775919732442, 0.9773760411588706]\n",
      "Training model for friend\n",
      "friend: [0.9933110367892977, 0.9866668158074294, 0.9933110367892977, 0.9899777782765818]\n",
      "Training model for humans\n",
      "humans: [0.9910813823857302, 0.9911611910444709, 0.9910813823857302, 0.9885532477618456]\n",
      "Training model for affect\n",
      "affect: [0.7993311036789298, 0.638930213308576, 0.7993311036789298, 0.7101863709266327]\n",
      "Training model for posemo\n",
      "posemo: [0.919732441471572, 0.8459077638952585, 0.919732441471572, 0.8812767296330393]\n",
      "Training model for negemo\n",
      "negemo: [0.8795986622073578, 0.7736938065569736, 0.8795986622073578, 0.823254263916495]\n",
      "Training model for anx\n",
      "anx: [0.9810479375696767, 0.9624550558097162, 0.9810479375696767, 0.9716625605642268]\n",
      "Training model for anger\n",
      "anger: [0.9498327759197325, 0.9021823022113847, 0.9498327759197325, 0.9253945398326039]\n",
      "Training model for sad\n",
      "sad: [0.9821627647714605, 0.9646436965035191, 0.9821627647714605, 0.9733244046835284]\n",
      "Training model for cogmech\n",
      "cogmech: [0.835005574136009, 0.8623506749510936, 0.835005574136009, 0.7641929481394365]\n",
      "Training model for insight\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "insight: [0.9565217391304348, 0.9149338374291116, 0.9565217391304348, 0.9352657004830918]\n",
      "Training model for cause\n",
      "cause: [0.9665551839464883, 0.9342289236138298, 0.9665551839464883, 0.950117170614065]\n",
      "Training model for discrep\n",
      "discrep: [0.9821627647714605, 0.9824823568920166, 0.9821627647714605, 0.9762976399675]\n",
      "Training model for tentat\n",
      "tentat: [0.9698996655518395, 0.9407053612375701, 0.9698996655518395, 0.9550794669271084]\n",
      "Training model for certain\n",
      "certain: [0.9821627647714605, 0.9824812868291128, 0.9821627647714605, 0.9743154525763221]\n",
      "Training model for inhib\n",
      "inhib: [0.9743589743589743, 0.9493754109138723, 0.9743589743589743, 0.9617049617049617]\n",
      "Training model for incl\n",
      "incl: [0.9955406911928651, 0.9911012678207677, 0.9955406911928651, 0.9933160192572386]\n",
      "Training model for excl\n",
      "excl: [0.9966555183946488, 0.9933222223465061, 0.9966555183946488, 0.9949860786653445]\n",
      "Training model for percept\n",
      "percept: [0.9587513935340022, 0.9604547400063703, 0.9587513935340022, 0.9396195612472089]\n",
      "Training model for see\n",
      "see: [0.9888517279821628, 0.9778277399333093, 0.9888517279821628, 0.9833088371302449]\n",
      "Training model for hear\n",
      "hear: [0.992196209587514, 0.98445331831983, 0.992196209587514, 0.9883095988056938]\n",
      "Training model for feel\n",
      "feel: [0.9899665551839465, 0.9900673375537505, 0.9899665551839465, 0.985887286894383]\n",
      "Training model for bio\n",
      "bio: [0.8807134894091416, 0.8949586419015766, 0.8807134894091416, 0.8259520409732457]\n",
      "Training model for body\n",
      "body: [0.9665551839464883, 0.9676749880554227, 0.9665551839464883, 0.9511626439246978]\n",
      "Training model for health\n",
      "health: [0.9498327759197325, 0.9021823022113847, 0.9498327759197325, 0.9253945398326039]\n",
      "Training model for sexual\n",
      "sexual: [0.9855072463768116, 0.9712245326612057, 0.9855072463768116, 0.978313762826616]\n",
      "Training model for ingest\n",
      "ingest: [0.9721293199554069, 0.9450354147169618, 0.9721293199554069, 0.9583909180340473]\n",
      "Training model for relativ\n",
      "relativ: [0.8483835005574136, 0.8713967192228061, 0.8483835005574136, 0.779899719408226]\n",
      "Training model for motion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "motion: [0.9531772575250836, 0.9085468842630396, 0.9531772575250836, 0.9303271177898933]\n",
      "Training model for space\n",
      "space: [0.9509476031215162, 0.953356426182513, 0.9509476031215162, 0.928105125398927]\n",
      "Training model for time\n",
      "time: [0.9464882943143813, 0.8958400912741469, 0.9464882943143813, 0.9204679975634704]\n",
      "Training model for work\n",
      "work: [0.9297658862876255, 0.8644646033042136, 0.9297658862876255, 0.8959269198889424]\n",
      "Training model for achieve\n",
      "achieve: [0.9498327759197325, 0.9021823022113847, 0.9498327759197325, 0.9253945398326039]\n",
      "Training model for leisure\n",
      "leisure: [0.955406911928651, 0.9128023673610413, 0.955406911928651, 0.933618840961065]\n",
      "Training model for home\n",
      "home: [0.9765886287625418, 0.9537253498283017, 0.9765886287625418, 0.9650215891663696]\n",
      "Training model for money\n",
      "money: [0.9609810479375697, 0.9234845744951895, 0.9609810479375697, 0.9418597650053269]\n",
      "Training model for relig\n",
      "relig: [0.9620958751393534, 0.9256284729601583, 0.9620958751393534, 0.9435099320968886]\n",
      "Training model for death\n",
      "death: [0.9866220735785953, 0.9734231160725272, 0.9866220735785953, 0.9799781538911974]\n",
      "Training model for assent\n",
      "assent: [0.9899665551839465, 0.9800337803827698, 0.9899665551839465, 0.9849751271746157]\n",
      "Training model for nonfl\n",
      "nonfl: [0.9955406911928651, 0.9911012678207677, 0.9955406911928651, 0.9933160192572386]\n",
      "Training model for filler\n",
      "filler: [0.9988851727982163, 0.9977715884361225, 0.9988851727982163, 0.9983280700805375]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/jessedoka/dev/LCT/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in category_names:\n",
    "    print(f'Training model for {i}')\n",
    "    print(f\"{i}: {train_model(liwc_lexicon, i, liwc_model, None)[1:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "# turn off warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X = liwc_lexicon['preprocessed_text']\n",
    "y = liwc_lexicon[category_names]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a list to store each category's model\n",
    "models = [(i, train_model(liwc_lexicon, i, liwc_model, None)[0]) for i in category_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.010033444816053512, Precision: 0.4604706862438821, Recall: 0.052014995313964384, F1: 0.08364318998361421\n"
     ]
    }
   ],
   "source": [
    "# Create an ensemble model\n",
    "ensemble = MultiOutputClassifier(VotingClassifier(estimators=models, voting='hard'))\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Use the ensemble model to make predictions\n",
    "predictions = ensemble.predict(X_test)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape () instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m stacking \u001b[38;5;241m=\u001b[39m StackingClassifier(estimators\u001b[38;5;241m=\u001b[39mmodels, final_estimator\u001b[38;5;241m=\u001b[39mLinearSVC(dual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Train the stacking model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mstacking\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use the stacking model to make predictions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m stacking\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:657\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    655\u001b[0m check_classification_targets(y)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_of_target(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mLabelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43myk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43myk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [le\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m le \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder]\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    660\u001b[0m         [\n\u001b[1;32m    661\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder[target_idx]\u001b[38;5;241m.\u001b[39mtransform(target)\n\u001b[1;32m    662\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m target_idx, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    663\u001b[0m         ]\n\u001b[1;32m    664\u001b[0m     )\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/ensemble/_stacking.py:657\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    655\u001b[0m check_classification_targets(y)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m type_of_target(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel-indicator\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder \u001b[38;5;241m=\u001b[39m [\u001b[43mLabelEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43myk\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m yk \u001b[38;5;129;01min\u001b[39;00m y\u001b[38;5;241m.\u001b[39mT]\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [le\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m le \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder]\n\u001b[1;32m    659\u001b[0m     y_encoded \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m    660\u001b[0m         [\n\u001b[1;32m    661\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label_encoder[target_idx]\u001b[38;5;241m.\u001b[39mtransform(target)\n\u001b[1;32m    662\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m target_idx, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(y\u001b[38;5;241m.\u001b[39mT)\n\u001b[1;32m    663\u001b[0m         ]\n\u001b[1;32m    664\u001b[0m     )\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:97\u001b[0m, in \u001b[0;36mLabelEncoder.fit\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, y):\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit label encoder.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \n\u001b[1;32m     87\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m        Fitted label encoder.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m _unique(y)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/dev/LCT/env/lib/python3.11/site-packages/sklearn/utils/validation.py:1367\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, dtype, warn)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1357\u001b[0m             (\n\u001b[1;32m   1358\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA column-vector y was passed when a 1d array was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1363\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1364\u001b[0m         )\n\u001b[1;32m   1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _asarray_with_order(xp\u001b[38;5;241m.\u001b[39mreshape(y, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)), order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m-> 1367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1368\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my should be a 1d array, got an array of shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(shape)\n\u001b[1;32m   1369\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: y should be a 1d array, got an array of shape () instead."
     ]
    }
   ],
   "source": [
    "# Create a stacking model\n",
    "stacking = StackingClassifier(estimators=models, final_estimator=LinearSVC(dual=True))\n",
    "\n",
    "# Train the stacking model\n",
    "stacking.fit(X_train, y_train)\n",
    "\n",
    "# Use the stacking model to make predictions\n",
    "\n",
    "predictions = stacking.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "\n",
    "accuracy, precision, recall, f1 = evaluate_model(y_test, predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
